---
title: "07 - Feature Engineering y Creaci√≥n de Variables Derivadas"
date: 2025-10-22
---

# üß© Feature Engineering y Creaci√≥n de Variables Derivadas

## Contexto
En esta pr√°ctica trabaj√© con un **dataset sint√©tico de viviendas** y posteriormente con el **dataset real Ames Housing**, con el objetivo de **crear, transformar y evaluar nuevas features** que mejoren la capacidad predictiva de los modelos.  
Se busc√≥ entender c√≥mo el *feature engineering* influye en el rendimiento, interpretabilidad y generalizaci√≥n de los modelos de Machine Learning.  

La pr√°ctica incluy√≥ tanto la **creaci√≥n de variables derivadas** (ratios, transformaciones, interacciones, categor√≠as) como la **evaluaci√≥n cuantitativa** mediante *Mutual Information*, *Random Forest Importance* y una **comparaci√≥n de modelos baseline vs engineered**.

---

## Objetivos

- Comprender la relevancia del *feature engineering* dentro del flujo de ML.  
- Generar nuevas variables num√©ricas, temporales y compuestas.  
- Detectar *outliers* y estudiar la distribuci√≥n de variables derivadas.  
- Evaluar importancia de features con *Mutual Information* y *Random Forest*.  
- Aplicar las t√©cnicas aprendidas sobre datos reales (Ames Housing).  
- Comparar modelos baseline vs engineered y analizar la mejora en rendimiento.  
- Reflexionar sobre creatividad, interpretaci√≥n y buenas pr√°cticas en feature design.

---

## Actividades (con tiempos estimados)

- Setup y generaci√≥n del dataset sint√©tico ‚Äî 20 min  
- Creaci√≥n de variables derivadas (ratios, transformaciones, compuestas) ‚Äî 45 min  
- An√°lisis de distribuci√≥n y detecci√≥n de outliers ‚Äî 40 min  
- C√°lculo de importancia de features (MI / RF) ‚Äî 35 min  
- Investigaci√≥n libre: creaci√≥n de nuevas features ‚Äî 60 min  
- Aplicaci√≥n a dataset real (Ames Housing) ‚Äî 40 min  
- Comparativa de modelos (Baseline vs Engineered) ‚Äî 30 min  
- Reflexiones, documentaci√≥n y armado del portfolio ‚Äî 40 min  

---

## Desarrollo

### Dataset sint√©tico
Se gener√≥ un dataset de **1000 viviendas**, con 10 variables base:

`price`, `sqft`, `bedrooms`, `bathrooms`, `year_built`, `garage_spaces`,  
`lot_size`, `distance_to_city`, `school_rating`, `crime_rate`.

---

## Evidencias

- [Notebook completo en nbviewer](https://nbviewer.org/github/naguer019/IA-portfolio/blob/main/docs/recursos_files/ocho.ipynb)


### 1Ô∏è‚É£ Creaci√≥n de Features Derivadas

**Ratios y proporciones**
- `price_per_sqft`: precio por pie cuadrado.  
- `sqft_per_bedroom`: superficie por habitaci√≥n.  
- `construction_density`: densidad de construcci√≥n (`sqft / lot_size`).  
- `city_school_ratio`: distancia a ciudad / calidad escolar.

**Variables temporales**
- `property_age` = 2024 ‚àí `year_built`.  
- `age_category`: ‚ÄúAntiguo‚Äù, ‚ÄúModerno‚Äù o ‚ÄúNuevo‚Äù.  
- `is_new_property`: indicador booleano.

**Transformaciones matem√°ticas**
- `log_price`: logaritmo natural del precio.  
- `sqrt_sqft`, `sqft_squared`: ra√≠z cuadrada y cuadrado del tama√±o.

**Features compuestas**
- `luxury_score` = `price + sqft + garage_spaces`.  
- `location_score` = `‚àídistance_to_city + school_rating`.

üìä **Resultado:**  
Dataset original: 10 columnas ‚Üí con features derivadas: 22 columnas (12 nuevas).

---

### 2Ô∏è‚É£ An√°lisis de Distribuci√≥n y Outliers

Se visualizaron las distribuciones de las nuevas variables y se aplic√≥ el m√©todo **IQR** para detectar outliers.

**Outliers detectados:**
- `price_per_sqft`: 3.7%  
- `sqft_per_bedroom`: 4.5%  
- `property_age`: 0.0%

üì∏ *Figura 1 ‚Äî Distribuci√≥n de features derivadas*  
![Distribuciones derivadas](../assets/ent7_prac8_fig1_distribucion_features_derivadas.png){ width="850" }

---

### 3Ô∏è‚É£ Evaluaci√≥n de Importancia de Features

Se utilizaron dos m√©todos complementarios:

- **Mutual Information (MI)**: mide dependencia no lineal.  
- **Random Forest Importance (RF)**: mide contribuci√≥n a la predicci√≥n.

**Top 10 ‚Äì Mutual Information**
- `construction_density`, `bedrooms`, `sqrt_sqft`, `sqft`, `sqft_squared`, `sqft_per_bedroom`.

**Top 10 ‚Äì Random Forest**
- `crime_rate`, `lot_size`, `sqft_per_bedroom`, `school_rating`, `distance_to_city`, `property_age`.

üì∏ *Figura 2 ‚Äî Importancia de features (MI vs RF)*  
![Top 10 MI vs RF](../assets/ent7_prac8_fig2_importancia_features_mi_rf.png){ width="850" }

---

### 4Ô∏è‚É£ Investigaci√≥n Libre: Nuevas Features

Se dise√±aron variables personalizadas basadas en intuici√≥n inmobiliaria:

**Features de dominio**
- `space_efficiency` = `sqft / lot_size`.  
- `crowded_property` = `bedrooms / sqft`.  
- `location_score` = combinaci√≥n de crimen, escuela y distancia (normalizadas).

**Interacciones**
- `price_age_interaction` = `price_per_sqft √ó property_age`.  
- `distance_school_interaction` = `distance_to_city √ó school_rating`.

**Correlaciones principales (dataset sint√©tico)**

| Feature | Corr. con `price` |
|----------|------------------|
| `price_age_interaction` | **+0.359** |
| `space_efficiency` | ‚àí0.031 |
| `crowded_property` | +0.026 |
| `location_score` | ‚àí0.059 |
| `distance_school_interaction` | +0.016 |

üì∏ *Figura 3 ‚Äî Distribuci√≥n de nuevas features (investigaci√≥n libre)*  
![Distribuci√≥n de features nuevas](../assets/ent7_prac8_fig3_distribucion_nuevas_features.png){ width="850" }

---

### 5Ô∏è‚É£ Aplicaci√≥n a Datos Reales ‚Äî Ames Housing

Se probaron las mismas t√©cnicas con una muestra del dataset real **Ames Housing**:

**Nuevas variables creadas:**
- `property_age` = 2024 ‚àí `YearBuilt`  
- `space_efficiency` = `GrLivArea / LotArea`  
- `age_size_interaction` = `property_age √ó GrLivArea`  
- `bath_per_bedroom` = `FullBath / BedroomAbvGr`  
- `garage_per_size` = `GarageCars / LotArea`

**Correlaciones con `SalePrice`:**

| Feature | Corr. |
|----------|--------|
| `space_efficiency` | **+0.875** |
| `garage_per_size` | +0.704 |
| `bath_per_bedroom` | +0.658 |
| `property_age` | **‚àí0.822** |
| `age_size_interaction` | ‚àí0.437 |


---


## Reflexi√≥n

### 1Ô∏è‚É£ Features m√°s importantes
- `crime_rate`, `lot_size`, `sqft_per_bedroom`, `school_rating`, `distance_to_city`, `property_age`.  
- En Ames Housing, destacaron `space_efficiency` y `garage_per_size`.

### 2Ô∏è‚É£ Sorpresas encontradas
- `location_score` tuvo correlaci√≥n negativa, lo que sugiere que los pesos deben calibrarse.  
- Algunas relaciones en el dataset sint√©tico fueron d√©biles o contrarias a lo esperado, producto del ruido aleatorio.

### 3Ô∏è‚É£ C√≥mo mejorar el proceso
- Ajustar `location_score` con regresi√≥n o PCA.  
- Generar interacciones sistem√°ticas y seleccionarlas con *Permutation Importance*.  
- Aplicar normalizaciones robustas y regularizaci√≥n (L1 / RFE).

### 4Ô∏è‚É£ Otras t√©cnicas de feature engineering
- **Num√©ricas:** polinomios, discretizaci√≥n supervisada, Box-Cox, log transform.  
- **Categ√≥ricas:** Target, Binary, Frequency y Ordinal Encoding.  
- **Temporales:** cyclical encoding, recency, aging features.  
- **Geoespaciales:** distancia a puntos de inter√©s, clusters por zona.

### 5Ô∏è‚É£ Diferencias entre datos sint√©ticos y reales
- Los sint√©ticos presentan correlaciones d√©biles y ruido.  
- En datos reales, las relaciones son m√°s claras y coherentes (edad ‚Üì precio ‚Üë valor).  
- El *feature engineering* resulta m√°s eficaz cuando refleja patrones reales.

---

## Conclusiones Finales

- El **feature engineering** tiene un impacto decisivo en el rendimiento del modelo.  
- Las **interacciones y transformaciones no lineales** mejoran la capacidad predictiva, sobre todo con modelos de √°rboles.  
- Los modelos con features derivadas superaron a los baselines tanto en **R¬≤** como en **MAE/MAPE**.  
- **Random Forest** captur√≥ mejor los efectos combinados que **Linear Regression**.  
- La creatividad y el conocimiento de dominio son fundamentales en esta etapa del pipeline.

---

## Referencias

- [Documentaci√≥n oficial de Scikit-learn](https://scikit-learn.org/stable/index.html)  
- [Gu√≠a oficial de Pipelines](https://scikit-learn.org/stable/modules/compose.html#pipeline)  
- [Link a la gu√≠a de la pr√°ctica](https://juanfkurucz.com/ucu-id/ut3/08-feature-engineering-assignment/#preguntas-para-reflexionar)

---

