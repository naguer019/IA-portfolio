---
title: "06 - Sesgo y Fairness con Fairlearn"
date: 2025-10-12
---

# ‚öñÔ∏è Pr√°ctica 7 ‚Äî Sesgo y Fairness con Fairlearn

## Contexto

En esta pr√°ctica se exploraron conceptos de **sesgo algor√≠tmico**, **equidad** y **responsabilidad √©tica** en el desarrollo de modelos de Machine Learning.  
Se trabaj√≥ con dos datasets emblem√°ticos ‚Äî**Boston Housing** (sesgo racial hist√≥rico) y **Titanic** (sesgo de g√©nero y clase)‚Äî para **detectar, cuantificar y mitigar** distintos tipos de sesgo usando la librer√≠a **Fairlearn**.  
El objetivo fue comprender no solo el aspecto t√©cnico del fairness, sino tambi√©n su dimensi√≥n √©tica y social en contextos reales.

---

## Objetivos

- Detectar sesgos en modelos de regresi√≥n y clasificaci√≥n.
- Aplicar m√©tricas de fairness como *Demographic Parity* y *Equalized Odds*.
- Usar herramientas de mitigaci√≥n de sesgo con **Fairlearn**.
- Reflexionar sobre la responsabilidad √©tica en el uso de datos sesgados.
- Comparar el impacto de modelos baseline vs. modelos ajustados por fairness.

---

## Actividades (con tiempos estimados)

- Revisi√≥n te√≥rica y carga de datasets ‚Äî 25 min  
- An√°lisis exploratorio y detecci√≥n de sesgo ‚Äî 45 min  
- Implementaci√≥n de mitigaci√≥n con Fairlearn ‚Äî 50 min  
- Reflexi√≥n √©tica y documentaci√≥n ‚Äî 40 min  
- Preparaci√≥n de entrega y subida al portfolio ‚Äî 50 min  

---

## Desarrollo

Se analizaron dos casos principales:

1. **Boston Housing (Regresi√≥n)** ‚Äî An√°lisis del sesgo racial codificado en la variable `B`, que refleja la proporci√≥n de residentes afroamericanos por vecindario.  
2. **Titanic (Clasificaci√≥n)** ‚Äî Evaluaci√≥n de la disparidad en la tasa de supervivencia por g√©nero y clase social, aplicando m√©todos de mitigaci√≥n con Fairlearn.

---

## Evidencias

- [Notebook completo en nbviewer](https://nbviewer.org/github/naguer019/IA-portfolio/blob/main/docs/recursos_files/siete.ipynb)

### üè† Caso 1: Boston Housing ‚Äì Detecci√≥n de Sesgo Racial

**C√≥mo se hizo:**  
Se carg√≥ el dataset desde la fuente original de CMU, reconstruyendo su formato especial y calculando una nueva variable `Bk_racial`, derivada de `B`, que representa la proporci√≥n de poblaci√≥n afroamericana.  
Luego, se dividi√≥ el conjunto en dos grupos:

- `Alta_prop_afroam`
- `Baja_prop_afroam`

y se compararon los precios medios de las viviendas entre ambos.

**Visualizaci√≥n:**

![Distribuci√≥n de precios por grupo racial](../assets/ent6-prac7-Distribuci√≥n-precios-grupo-racial.png){ width="700" }

**Qu√© muestra:**  
Las distribuciones de precios son **muy similares** entre grupos.  
- Media Alta_prop_afroam = 22.81k  
- Media Baja_prop_afroam = 22.25k  
- Brecha promedio: **‚àí0.56 k (‚àí2.4%)**

El **boxplot** refleja **medianas casi id√©nticas**, aunque el grupo con baja proporci√≥n afroamericana presenta **mayor dispersi√≥n** y m√°s valores altos.  
**Conclusi√≥n:** no hay evidencia de un sesgo fuerte en los precios medios, pero s√≠ diferencias leves en la variabilidad.

---

### ‚öñÔ∏è Reflexi√≥n √âtica sobre el uso de la variable B

**Preguntas gu√≠a y respuestas:**

1. **¬øEs √©tico usar la variable B en 2025?**  
   ‚ùå No. Representa directamente un componente racial y fue dise√±ada en 1978 en un contexto de segregaci√≥n.

2. **¬øCu√°ndo la utilidad justifica el sesgo?**  
   Solo en casos educativos, donde el objetivo es **comprender el sesgo** y no producir decisiones automatizadas.

3. **Responsabilidad profesional:**  
   Los data scientists deben evitar el uso de datos sesgados fuera de fines acad√©micos y **documentar expl√≠citamente** las limitaciones √©ticas.

4. **Decisi√≥n final:**  
   **‚ÄúUSAR SOLO PARA EDUCACI√ìN ‚Äì NO PARA PRODUCCI√ìN‚Äù**  
   Justificaci√≥n: Variable hist√≥ricamente sesgada, √∫til para aprender, pero inapropiada para modelos reales.

**Correlaciones alternativas sin sesgo expl√≠cito:**
| Variable | Correlaci√≥n con MEDV |
|-----------|----------------------|
| LSTAT | -0.738 |
| RM | +0.695 |
| CRIM | -0.388 |
| TAX | -0.469 |
| PTRATIO | -0.508 |

Estas variables explican el valor de la vivienda **sin implicaciones raciales directas**.

---

### üö¢ Caso 2: Titanic ‚Äì Sesgo por G√©nero y Clase

**C√≥mo se hizo:**  
Se utiliz√≥ el dataset Titanic de Seaborn y se analizaron tasas de supervivencia por g√©nero y clase.  
Luego, se aplic√≥ la t√©cnica de mitigaci√≥n **ExponentiatedGradient** de Fairlearn con la restricci√≥n **Demographic Parity**.

**Resultados principales:**

| Modelo | Accuracy | Demographic Parity Diff |
|---------|-----------|--------------------------|
| Baseline (RandomForest) | 0.673 | 0.113 |
| Fairlearn (Mitigado) | 0.626 | 0.055 |

**Trade-off:**
- Performance loss: **‚âà 6.9%**
- Fairness gain: **‚âà 0.06**

**Interpretaci√≥n:**  
El modelo mitigado reduce significativamente la brecha de equidad con una p√©rdida de rendimiento menor al 7%.  
Esto representa un **buen compromiso (trade-off)** entre **precisi√≥n y justicia**.

---

### üí° An√°lisis Comparativo

| Dataset | Tipo | Fuente de Sesgo | Brecha Detectada | Estrategia | Resultado √âtico |
|----------|------|-----------------|------------------|-------------|-----------------|
| Boston Housing | Regresi√≥n | Racial (variable B) | ‚àí2.4% en precios medios | Detecci√≥n y documentaci√≥n | ‚ùå No usar en producci√≥n |
| Titanic | Clasificaci√≥n | G√©nero y clase | Gender gap: 54.8%, Class gap: 41.3% | Fairlearn (Demographic Parity) | ‚úÖ Uso mitigado posible |

---

## Reflexi√≥n final

**Lo aprendido:**
- Detectar sesgo es **tan importante como mitigarlo**.  
- No todos los sesgos son corregibles: algunos, como el racial en Boston Housing, deben ser **solo estudiados**, no ‚Äúarreglados‚Äù.  
- Fairlearn permite medir y mitigar sesgo, pero no elimina causas hist√≥ricas o estructurales.  
- La **documentaci√≥n y transparencia** son esenciales para un uso √©tico de los modelos.

**Conclusiones personales:**
- En contextos sensibles (cr√©dito, vivienda, justicia), el umbral de tolerancia debe ser **mucho m√°s estricto**.  
- Un modelo con **sesgo conocido y documentado** es preferible a uno ‚Äúcorregido‚Äù sin control ni trazabilidad.  
- La mitigaci√≥n del sesgo implica **aceptar p√©rdidas de performance** a cambio de **ganancias en equidad social**.

---

## Recomendaciones √©ticas y t√©cnicas

### Framework de decisi√≥n

| Situaci√≥n | Acci√≥n recomendada |
|------------|-------------------|
| Sesgo hist√≥rico complejo | Detectar y documentar |
| Sesgo sistem√°tico (ej. g√©nero) | Detectar + corregir |
| Impacto alto en personas | Rechazar el modelo |
| Contexto educativo | Usar para aprendizaje |

---

## Checklist final para proyectos futuros

1. ¬øLas features est√°n en escalas muy diferentes?  
2. ¬øMi proceso necesita fairness adem√°s de precisi√≥n?  
3. ¬øHay outliers o proxies que representen atributos sensibles?  
4. ¬øEstoy usando `Pipeline` y `cross-validation` para evitar leakage?  
5. ¬øHe documentado decisiones √©ticas y t√©cnicas?  
6. ¬øEl modelo es reproducible y auditable?  
7. ¬øExisten mecanismos de monitoreo de sesgo post-deployment?

---

## Regla de oro personal

> ‚ÄúSi hay duda, usa **Fairlearn** con m√©tricas de equidad, valida con **cross-validation**, y documenta siempre tus decisiones √©ticas.‚Äù

---

## Referencias

- [Fairlearn Documentation](https://fairlearn.org/)  
- [Scikit-learn Ethics Statement](https://scikit-learn.org/stable/whats_new/v0.24.html#ethics)  
- Dataset: *Boston Housing* (CMU) y *Titanic* (Seaborn)  
- [Link a la pr√°ctica 7 original](https://juanfkurucz.com/ucu-id/ut2/07-sesgo-y-fairness/)

---
