---
title: "üìâ Reducci√≥n inteligente de dimensiones: PCA y Feature Selection en Ames Housing"
date: 2025-11-06
---

# üìâ Reducci√≥n inteligente de dimensiones: PCA y Feature Selection en Ames Housing

## Contexto  
En esta pr√°ctica trabaj√© con el dataset **Ames Housing**, aplicando **reducci√≥n de dimensionalidad con PCA** y t√©cnicas de **selecci√≥n de features** (Filter, Wrapper y Embedded).  
El prop√≥sito fue mantener el rendimiento predictivo, reduciendo la cantidad de variables y mejorando la **interpretabilidad del modelo** para estimar precios de viviendas.

---

## Objetivos
- Aplicar **PCA** para explorar la estructura de varianza de los datos.  
- Identificar **features originales** m√°s representativas mediante *loadings*.  
- Comparar m√©todos de selecci√≥n: **Filter (F-test, MI)**, **Wrapper (SFS, RFE)** y **Embedded (RF, Lasso)**.  
- Evaluar el equilibrio entre **precisi√≥n, simplicidad e interpretabilidad**.

---

## Actividades (con tiempos estimados)
- Preprocesamiento y encoding ‚Äî 15 min  
- PCA y varianza explicada ‚Äî 20 min  
- Selecci√≥n interpretable por *loadings* ‚Äî 25 min  
- M√©todos Filter (F-test, MI) ‚Äî 25 min  
- M√©todos Wrapper (Forward, Backward, RFE) ‚Äî 45‚Äì60 min  
- M√©todos Embedded (RF, Lasso) ‚Äî 30 min  
- Comparativas y documentaci√≥n ‚Äî 30 min  

---

## Desarrollo

### **1. Dataset y preprocesamiento**  
- Dataset: **Ames Housing**, 2.930 filas y 81 columnas.  
- Limpieza: imputaci√≥n por mediana (num√©ricas) y moda (categ√≥ricas).  
- Categ√≥ricas codificadas con `LabelEncoder`.  
- Target: `SalePrice`.  
- Estandarizaci√≥n de features con media 0 y desviaci√≥n est√°ndar 1 para PCA.

---

### **2. PCA ‚Äî Varianza explicada**  
Se aplic√≥ **PCA** a todas las variables num√©ricas estandarizadas.

- PC1 explica **13.4%** y PC2 **5.0%** de la varianza.  
- Para alcanzar **80%** de varianza acumulada se necesitan **38 componentes**.  
- Se decidi√≥ conservar esos 38 para lograr un balance entre compresi√≥n e informaci√≥n.

![Scree plot y varianza acumulada](../assets/10-scree_plot.png){ width="800" }  
*Fig. 1 ‚Äî Scree plot y varianza acumulada: el ‚Äúcodo‚Äù se estabiliza cerca del componente 38.*

---

### **3. PCA Loadings ‚Äî Selecci√≥n interpretable de variables originales**
En lugar de usar los componentes transformados, se seleccionaron las **variables originales m√°s influyentes** seg√∫n la magnitud de sus *loadings* en PC1 y PC2.

Top variables:
- `Gr Liv Area`, `TotRms AbvGrd`, `2nd Flr SF`, `BsmtFin SF 1`, `Full Bath`.

Se retuvieron **38 variables originales**, igualando la reducci√≥n del PCA pero sin perder interpretabilidad.

![Importancia por PCA loadings](../assets/10-pca_loadings_importance.png){ width="800" }  
*Fig. 2 ‚Äî Ranking de variables seg√∫n la magnitud de sus loadings en PC1 y PC2.*

---

### **4. M√©todos Filter**  
Dos enfoques univariados para evaluar correlaci√≥n (lineal y no lineal) entre features y `SalePrice`.

#### a) F-test (ANOVA)
Eval√∫a relaci√≥n **lineal** entre cada variable y el target.  
- **RMSE = $26,395**  
- **R¬≤ = 0.8883**

![F-test (ANOVA)](../assets/10-f_test_scores.png){ width="800" }  
*Fig. 3 ‚Äî F-scores: fuerza lineal de relaci√≥n entre cada feature y el precio.*

#### b) Mutual Information (MI)
Captura dependencias **no lineales** con el target.  
- **RMSE = $26,279**  
- **R¬≤ = 0.8891**

![Mutual Information](../assets/10-mi_scores.png){ width="800" }  
*Fig. 4 ‚Äî MI Scores: variables con dependencia no lineal significativa con `SalePrice`.*

---

### **5. M√©todos Wrapper**  
Eval√∫an subconjuntos de variables a trav√©s del rendimiento del modelo (Regresi√≥n Lineal).

| M√©todo        | N¬∫ features | RMSE ($) | R¬≤     |
|----------------|-------------|----------|-------:|
| Forward        | 19          | 27,036   | 0.8828 |
| Backward       | 19          | 27,084   | 0.8827 |
| RFE            | 19          | 27,557   | 0.8782 |

![Ranking RFE](../assets/10-rfe_ranking.png){ width="800" }  
*Fig. 5 ‚Äî Ranking de variables seg√∫n RFE (1 = seleccionada).*

![Comparaci√≥n Forward vs RFE](../assets/10-forward_rfe_comparison.png){ width="800" }  
*Fig. 6 ‚Äî Forward (m√°s exhaustivo pero m√°s lento) vs RFE (eficiente en tiempo).*

---

### **6. M√©todos Embedded** 

#### a) Random Forest Feature Importance  
Basado en la ganancia de pureza media (reducci√≥n de MSE).  
- **RMSE = $26,238**, **R¬≤ = 0.8894**  
- Features m√°s importantes: `Overall Qual`, `Gr Liv Area`, `Total Bsmt SF`, `1st Flr SF`.

![Random Forest Feature Importance](../assets/10-rf_importance.png){ width="800" }  
*Fig. 7 ‚Äî Importancia global de las variables seg√∫n Random Forest.*

#### b) Lasso Regression  
Aplica regularizaci√≥n L1 para eliminar coeficientes peque√±os.  
- **Œ± ‚âà 1375**, **RMSE = $26,090**, **R¬≤ = 0.8908**  
- Elimin√≥ 41/81 variables ‚Üí fuerte **reducci√≥n y estabilidad**.

![Coeficientes Lasso](../assets/10-lasso_coefficients.png){ width="800" }  
*Fig. 8 ‚Äî Coeficientes m√°s influyentes (valores altos indican mayor peso predictivo).*

---

### **7. Comparativa general y hallazgos**  

| M√©todo / T√©cnica        | N¬∫ Feats | RMSE ($) | R¬≤     | Observaciones |
|--------------------------|----------|----------|-------:|---------------|
| Base (todas)            | 81       | 26,342   | 0.8885 | Referencia base |
| PCA (38 comps)           | 38       | 26,620   | 0.8859 | Menos interpretable |
| PCA-loadings (38 feats)  | 38       | 27,020   | 0.8830 | Interpretable |
| F-test (38 feats)        | 38       | 26,395   | 0.8883 | Buena correlaci√≥n lineal |
| MI (38 feats)            | 38       | 26,279   | 0.8891 | Captura no linealidad |
| RF Importance (38 feats) | 38       | 26,238   | 0.8894 | Alta estabilidad |
| Lasso (38 feats)         | 38       | 26,090   | 0.8908 | Mejor balance |
| Wrapper (19 feats)       | 19       | 27,036   | 0.8828 | M√°s simple y explicable |

**Conclusi√≥n:**  
- **Embedded y Filter** lograron el mejor rendimiento.  
- **Wrapper** produce modelos m√°s compactos e interpretables.  
- **PCA-loadings** ofrece equilibrio entre reducci√≥n y transparencia.  

---

## Evidencias  

- [Notebook completo](../recursos_files/Practica_10.ipynb)

## Reflexi√≥n

- **¬øPCA o selecci√≥n?**
PCA es √∫til para exploraci√≥n, pero las selecciones basadas en features originales (Lasso, RF) conservan significado interpretativo.


- **Variables m√°s relevantes:**
Overall Qual, Gr Liv Area, Total Bsmt SF, Garage Area, Year Built.


- **Colinealidad:**
Lasso elimin√≥ ~50% de variables redundantes.


- **En producci√≥n:**
Usar pipeline con Lasso o RF Importance (k‚âà38) y monitorear drift trimestral.

## Conclusiones

- Se logr√≥ reducir 53% de variables sin p√©rdida significativa de rendimiento.

- Lasso ofrece la mejor combinaci√≥n de precisi√≥n y simplicidad.

- RF Importance es estable y no lineal; complementa a Lasso.

- Wrapper favorece interpretabilidad, aunque a mayor costo computacional.

## Referencias

- Documentaci√≥n de Scikit-learn:  
  - [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)  
  - [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)  
  - [RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)  
  - [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)  
  - [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)  
- Blog y gu√≠a de referencia:  
  - [Feature Selection Methods ‚Äî Towards Data Science](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)  
  - [An Introduction to PCA ‚Äî Scikit-learn User Guide](https://scikit-learn.org/stable/modules/decomposition.html#pca)  