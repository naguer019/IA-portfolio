{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2f58f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ASSIGNMENT UT3-10: PCA y Feature Selection - Ames Housing Dataset\n",
      "================================================================================\n",
      "\n",
      "üè† CARGANDO Y PREPROCESANDO AMES HOUSING...\n",
      "‚úÖ Dataset cargado: 2,930 casas, 82 columnas\n",
      "‚úÖ Features num√©ricas: 38\n",
      "‚úÖ Features categ√≥ricas: 43\n",
      "‚úÖ Missing values imputados\n",
      "‚úÖ Categ√≥ricas encoded\n",
      "\n",
      "‚úÖ DATASET LISTO:\n",
      "   X shape: (2930, 81) (81 features)\n",
      "   y shape: (2930,)\n",
      "   Precio promedio: $180,796\n",
      "   Precio mediana: $160,000\n",
      "\n",
      "üìä RESUMEN DEL DATASET:\n",
      "   Total features: 81\n",
      "   Total casas: 2,930\n",
      "   Ejemplos de features: ['Order', 'PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area', 'Street', 'Alley', 'Lot Shape', 'Land Contour']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ASSIGNMENT UT3-10: PCA y Feature Selection - Ames Housing Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== FUNCI√ìN DE CARGA Y PREPROCESAMIENTO R√ÅPIDO ==========\n",
    "def quick_load_and_preprocess_ames(filepath='AmesHousing.csv'):\n",
    "    \"\"\"\n",
    "    Carga y preprocesa Ames Housing en un solo paso\n",
    "    (Ya hiciste esto en tareas anteriores, aqu√≠ es versi√≥n simplificada)\n",
    "    \"\"\"\n",
    "    print(\"\\nüè† CARGANDO Y PREPROCESANDO AMES HOUSING...\")\n",
    "\n",
    "    # Cargar dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"‚úÖ Dataset cargado: {df.shape[0]:,} casas, {df.shape[1]} columnas\")\n",
    "\n",
    "    # Eliminar 'Id' (no predictivo)\n",
    "    df = df.drop('Id', axis=1, errors='ignore')\n",
    "\n",
    "    # Identificar tipos de variables\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Remover target de features\n",
    "    if 'SalePrice' in numerical_cols:\n",
    "        numerical_cols.remove('SalePrice')\n",
    "\n",
    "    print(f\"‚úÖ Features num√©ricas: {len(numerical_cols)}\")\n",
    "    print(f\"‚úÖ Features categ√≥ricas: {len(categorical_cols)}\")\n",
    "\n",
    "    # Imputar valores faltantes\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    df[numerical_cols] = num_imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "    print(f\"‚úÖ Missing values imputados\")\n",
    "\n",
    "    # Label encoding para categ√≥ricas\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "    print(f\"‚úÖ Categ√≥ricas encoded\")\n",
    "\n",
    "    # Separar X y y\n",
    "    X = df.drop('SalePrice', axis=1)\n",
    "    y = df['SalePrice']\n",
    "\n",
    "    print(f\"\\n‚úÖ DATASET LISTO:\")\n",
    "    print(f\"   X shape: {X.shape} ({X.shape[1]} features)\")\n",
    "    print(f\"   y shape: {y.shape}\")\n",
    "    print(f\"   Precio promedio: ${y.mean():,.0f}\")\n",
    "    print(f\"   Precio mediana: ${y.median():,.0f}\")\n",
    "\n",
    "    return X, y, X.columns.tolist()\n",
    "\n",
    "# ========== EJECUTAR CARGA R√ÅPIDA ==========\n",
    "X, y, feature_names = quick_load_and_preprocess_ames(\"../../UT2/AmesHousing.csv\")\n",
    "\n",
    "print(f\"\\nüìä RESUMEN DEL DATASET:\")\n",
    "print(f\"   Total features: {X.shape[1]}\")\n",
    "print(f\"   Total casas: {X.shape[0]:,}\")\n",
    "print(f\"   Ejemplos de features: {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45e3d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESTANDARIZACI√ìN DE FEATURES ===\n",
      "‚ö†Ô∏è PCA es sensible a escala. SIEMPRE estandarizar antes de PCA.\n",
      "\n",
      "‚úÖ Mean despu√©s de scaling: -0.000000 (esperado: ~0)\n",
      "‚úÖ Std despu√©s de scaling: 1.000000 (esperado: ~1)\n",
      "‚úÖ X_scaled shape: (2930, 81)\n",
      "\n",
      "=== COMPARACI√ìN ANTES vs DESPU√âS ===\n",
      "Antes - Mean GrLivArea: 1500, Std: 506\n",
      "Despu√©s - Mean GrLivArea: 0.000000, Std: 1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ========== ESTANDARIZACI√ìN ==========\n",
    "print(\"=== ESTANDARIZACI√ìN DE FEATURES ===\")\n",
    "print(\"‚ö†Ô∏è PCA es sensible a escala. SIEMPRE estandarizar antes de PCA.\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Verificar estandarizaci√≥n: mean ‚âà 0, std ‚âà 1\n",
    "print(f\"\\n‚úÖ Mean despu√©s de scaling: {X_scaled.mean():.6f} (esperado: ~0)\")\n",
    "print(f\"‚úÖ Std despu√©s de scaling: {X_scaled.std():.6f} (esperado: ~1)\")\n",
    "\n",
    "# Verificar shape\n",
    "print(f\"‚úÖ X_scaled shape: {X_scaled.shape}\")\n",
    "\n",
    "# Comparar antes vs despu√©s\n",
    "print(f\"\\n=== COMPARACI√ìN ANTES vs DESPU√âS ===\")\n",
    "print(f\"Antes - Mean GrLivArea: {X['Gr Liv Area'].mean():.0f}, Std: {X['Gr Liv Area'].std():.0f}\")\n",
    "print(f\"Despu√©s - Mean GrLivArea: {X_scaled[:, X.columns.get_loc('Gr Liv Area')].mean():.6f}, Std: {X_scaled[:, X.columns.get_loc('Gr Liv Area')].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "600ddbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== APLICANDO PCA ===\n",
      "‚è±Ô∏è Esto puede tomar 10-20 segundos con 80 features...\n",
      "‚úÖ PCA completado en 0.10 segundos\n",
      "\n",
      "=== AN√ÅLISIS DE COMPONENTES PRINCIPALES ===\n",
      "Total de componentes generados: 81\n",
      "\n",
      "Varianza explicada por componentes principales:\n",
      "  PC1: 13.409% (¬°la m√°s importante!)\n",
      "  PC2: 4.956%\n",
      "  PC3: 4.709%\n",
      "  PC4: 3.690%\n",
      "  PC5: 2.974%\n",
      "\n",
      "=== TOP 10 COMPONENTES ===\n",
      "PC 1: Individual 13.409% | Acumulada 13.409%\n",
      "PC 2: Individual 4.956% | Acumulada 18.365%\n",
      "PC 3: Individual 4.709% | Acumulada 23.074%\n",
      "PC 4: Individual 3.690% | Acumulada 26.765%\n",
      "PC 5: Individual 2.974% | Acumulada 29.739%\n",
      "PC 6: Individual 2.727% | Acumulada 32.466%\n",
      "PC 7: Individual 2.550% | Acumulada 35.017%\n",
      "PC 8: Individual 2.404% | Acumulada 37.420%\n",
      "PC 9: Individual 2.239% | Acumulada 39.659%\n",
      "PC10: Individual 2.121% | Acumulada 41.780%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "# ========== APLICAR PCA SIN RESTRICCIONES ==========\n",
    "print(\"\\n=== APLICANDO PCA ===\")\n",
    "print(\"‚è±Ô∏è Esto puede tomar 10-20 segundos con 80 features...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pca = PCA()  # Sin n_components = todos los componentes posibles\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"‚úÖ PCA completado en {elapsed_time:.2f} segundos\")\n",
    "\n",
    "# ========== ANALIZAR VARIANZA EXPLICADA ==========\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"\\n=== AN√ÅLISIS DE COMPONENTES PRINCIPALES ===\")\n",
    "print(f\"Total de componentes generados: {pca.n_components_}\")\n",
    "print(f\"\\nVarianza explicada por componentes principales:\")\n",
    "print(f\"  PC1: {explained_variance[0]:.3%} (¬°la m√°s importante!)\")\n",
    "print(f\"  PC2: {explained_variance[1]:.3%}\")\n",
    "print(f\"  PC3: {explained_variance[2]:.3%}\")\n",
    "print(f\"  PC4: {explained_variance[3]:.3%}\")\n",
    "print(f\"  PC5: {explained_variance[4]:.3%}\")\n",
    "\n",
    "print(\"\\n=== TOP 10 COMPONENTES ===\")\n",
    "for i in range(min(10, len(explained_variance))):\n",
    "    print(f\"PC{i+1:2d}: Individual {explained_variance[i]:6.3%} | Acumulada {cumulative_variance[i]:6.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab226bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SCREE PLOT: VISUALIZACI√ìN DE VARIANZA ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DECISI√ìN: ¬øCU√ÅNTOS COMPONENTES NECESITAMOS? ===\n",
      "üìä Para 80% de varianza: 38 componentes\n",
      "üìä Para 90% de varianza: 51 componentes\n",
      "üìä Para 95% de varianza: 59 componentes\n",
      "\n",
      "üéØ IMPACTO DE REDUCCI√ìN DIMENSIONAL:\n",
      "   Original: 81 features\n",
      "   80% varianza: 81 ‚Üí 38 (53.1% reducci√≥n)\n",
      "   90% varianza: 81 ‚Üí 51 (37.0% reducci√≥n)\n",
      "   95% varianza: 81 ‚Üí 59 (27.2% reducci√≥n)\n",
      "\n",
      "üí° RECOMENDACI√ìN PR√ÅCTICA:\n",
      "   Para este assignment, usaremos 38 componentes (80% varianza)\n",
      "   Esto balancea reducci√≥n dimensional con retenci√≥n de informaci√≥n\n"
     ]
    }
   ],
   "source": [
    "# ========== CREAR SCREE PLOT ==========\n",
    "print(\"\\n=== SCREE PLOT: VISUALIZACI√ìN DE VARIANZA ===\")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Varianza individual (primeros 30 componentes para claridad)\n",
    "plt.subplot(1, 2, 1)\n",
    "n_to_show = min(30, len(explained_variance))\n",
    "plt.bar(range(1, n_to_show + 1), explained_variance[:n_to_show], alpha=0.7, color='steelblue')\n",
    "plt.xlabel('Componente Principal', fontsize=12)\n",
    "plt.ylabel('Varianza Explicada (Individual)', fontsize=12)\n",
    "plt.title(f'Scree Plot - Primeros {n_to_show} Componentes', fontsize=14)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 2: Varianza acumulada (TODOS los componentes)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', \n",
    "         color='steelblue', markersize=4, linewidth=2)\n",
    "\n",
    "# L√≠neas de referencia\n",
    "plt.axhline(y=0.80, color='r', linestyle='--', label='80% varianza', linewidth=2)\n",
    "plt.axhline(y=0.90, color='g', linestyle='--', label='90% varianza', linewidth=2)\n",
    "plt.axhline(y=0.95, color='orange', linestyle='--', label='95% varianza', linewidth=2)\n",
    "\n",
    "plt.xlabel('N√∫mero de Componentes', fontsize=12)\n",
    "plt.ylabel('Varianza Acumulada', fontsize=12)\n",
    "plt.title('Varianza Acumulada por N√∫mero de Componentes', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-scree_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ========== DECISI√ìN DE DIMENSIONALIDAD ==========\n",
    "print(\"\\n=== DECISI√ìN: ¬øCU√ÅNTOS COMPONENTES NECESITAMOS? ===\")\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) \n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90)\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) \n",
    "\n",
    "print(f\"üìä Para 80% de varianza: {n_components_80} componentes\")\n",
    "print(f\"üìä Para 90% de varianza: {n_components_90} componentes\")\n",
    "print(f\"üìä Para 95% de varianza: {n_components_95} componentes\")\n",
    "\n",
    "# An√°lisis de reducci√≥n dimensional\n",
    "original_features = X.shape[1]\n",
    "reduction_80 = (1 - n_components_80 / original_features) * 100\n",
    "reduction_90 = (1 - n_components_90 / original_features) * 100\n",
    "reduction_95 = (1 - n_components_95 / original_features) * 100\n",
    "\n",
    "print(f\"\\nüéØ IMPACTO DE REDUCCI√ìN DIMENSIONAL:\")\n",
    "print(f\"   Original: {original_features} features\")\n",
    "print(f\"   80% varianza: {original_features} ‚Üí {n_components_80} ({reduction_80:.1f}% reducci√≥n)\")\n",
    "print(f\"   90% varianza: {original_features} ‚Üí {n_components_90} ({reduction_90:.1f}% reducci√≥n)\")\n",
    "print(f\"   95% varianza: {original_features} ‚Üí {n_components_95} ({reduction_95:.1f}% reducci√≥n)\")\n",
    "\n",
    "print(f\"\\nüí° RECOMENDACI√ìN PR√ÅCTICA:\")\n",
    "print(f\"   Para este assignment, usaremos {n_components_80} componentes (80% varianza)\")\n",
    "print(f\"   Esto balancea reducci√≥n dimensional con retenci√≥n de informaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b0e42f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE SELECTION BASADA EN PCA LOADINGS ===\n",
      "üí° En lugar de usar PC1, PC2... usaremos las features ORIGINALES\n",
      "   que tienen mayor loading (peso) en los componentes principales\n",
      "\n",
      "üîç Analizando loadings de los primeros 2 componentes...\n",
      "\n",
      "üìä ESTRATEGIA: Ranking de features por suma de loadings absolutos\n",
      "\n",
      "üîù TOP 20 FEATURES POR IMPORTANCIA EN PCA:\n",
      "   1. Gr Liv Area              : 0.4494\n",
      "   2. TotRms AbvGrd            : 0.4431\n",
      "   3. 2nd Flr SF               : 0.4354\n",
      "   4. BsmtFin SF 1             : 0.3946\n",
      "   5. Full Bath                : 0.3841\n",
      "   6. Bsmt Full Bath           : 0.3415\n",
      "   7. Year Built               : 0.3399\n",
      "   8. Bedroom AbvGr            : 0.3335\n",
      "   9. Total Bsmt SF            : 0.3159\n",
      "  10. BsmtFin Type 1           : 0.3014\n",
      "  11. Half Bath                : 0.2913\n",
      "  12. Overall Qual             : 0.2847\n",
      "  13. Bsmt Unf SF              : 0.2821\n",
      "  14. 1st Flr SF               : 0.2728\n",
      "  15. Bsmt Exposure            : 0.2628\n",
      "  16. Garage Type              : 0.2559\n",
      "  17. Garage Yr Blt            : 0.2553\n",
      "  18. Garage Cars              : 0.2512\n",
      "  19. Garage Finish            : 0.2456\n",
      "  20. Paved Drive              : 0.2406\n",
      "\n",
      "‚úÖ Seleccionando top 38 features originales basadas en loadings de PCA\n",
      "\n",
      "üìã Features seleccionadas (38):\n",
      "   1. Gr Liv Area\n",
      "   2. TotRms AbvGrd\n",
      "   3. 2nd Flr SF\n",
      "   4. BsmtFin SF 1\n",
      "   5. Full Bath\n",
      "   6. Bsmt Full Bath\n",
      "   7. Year Built\n",
      "   8. Bedroom AbvGr\n",
      "   9. Total Bsmt SF\n",
      "  10. BsmtFin Type 1\n",
      "  11. Half Bath\n",
      "  12. Overall Qual\n",
      "  13. Bsmt Unf SF\n",
      "  14. 1st Flr SF\n",
      "  15. Bsmt Exposure\n",
      "  16. Garage Type\n",
      "  17. Garage Yr Blt\n",
      "  18. Garage Cars\n",
      "  19. Garage Finish\n",
      "  20. Paved Drive\n",
      "  21. Bsmt Qual\n",
      "  22. Exter Qual\n",
      "  23. Kitchen Qual\n",
      "  24. Garage Area\n",
      "  25. Heating QC\n",
      "  26. Year Remod/Add\n",
      "  27. Central Air\n",
      "  28. House Style\n",
      "  29. Open Porch SF\n",
      "  30. Garage Qual\n",
      "  31. Foundation\n",
      "  32. Electrical\n",
      "  33. Kitchen AbvGr\n",
      "  34. Enclosed Porch\n",
      "  35. Wood Deck SF\n",
      "  36. Mas Vnr Area\n",
      "  37. BsmtFin Type 2\n",
      "  38. Exterior 2nd\n",
      "\n",
      "‚úÖ Dataset con features seleccionadas por PCA:\n",
      "   Shape: (2930, 38)\n",
      "   Reducci√≥n: 81 ‚Üí 38 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° INTERPRETACI√ìN:\n",
      "   Estas features originales son las que 'explican' los componentes principales\n",
      "   Ventaja: Mantienen interpretabilidad (puedes decir 'GrLivArea importa')\n",
      "   Diferencia con PCA: Usas features originales, no combinaciones lineales\n"
     ]
    }
   ],
   "source": [
    "# ========== FEATURE SELECTION BASADA EN PCA LOADINGS ==========\n",
    "print(\"\\n=== FEATURE SELECTION BASADA EN PCA LOADINGS ===\")\n",
    "print(\"üí° En lugar de usar PC1, PC2... usaremos las features ORIGINALES\")\n",
    "print(\"   que tienen mayor loading (peso) en los componentes principales\")\n",
    "\n",
    "# Decidir cu√°ntos componentes considerar\n",
    "n_top_components = 2\n",
    "\n",
    "# Obtener loadings absolutos de todos los componentes importantes\n",
    "print(f\"\\nüîç Analizando loadings de los primeros {n_top_components} componentes...\")\n",
    "\n",
    "# Para cada componente, obtener las features con mayor loading absoluto\n",
    "all_loadings = pca.components_[:n_top_components, :]  # Primeros n componentes\n",
    "\n",
    "# Crear DataFrame con loadings de todos los componentes\n",
    "loadings_all = pd.DataFrame(\n",
    "    all_loadings.T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_top_components)],\n",
    "    index=X.columns\n",
    ")\n",
    "\n",
    "# ========== ESTRATEGIA: SUMAR LOADINGS ABSOLUTOS ==========\n",
    "# Para cada feature, sumar su importancia (loading absoluto) en todos los componentes\n",
    "print(\"\\nüìä ESTRATEGIA: Ranking de features por suma de loadings absolutos\")\n",
    "\n",
    "feature_importance_from_pca = loadings_all.abs().sum(axis=1)\n",
    "\n",
    "# Ordenar por importancia\n",
    "feature_importance_from_pca = feature_importance_from_pca.sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nüîù TOP 20 FEATURES POR IMPORTANCIA EN PCA:\")\n",
    "for i, (feature, importance) in enumerate(feature_importance_from_pca.head(20).items(), 1):\n",
    "    print(f\"  {i:2d}. {feature:25s}: {importance:.4f}\")\n",
    "\n",
    "# ========== SELECCIONAR TOP-K FEATURES ==========\n",
    "k = n_components_80  # Mismo n√∫mero que usamos con PCA reducido\n",
    "\n",
    "print(f\"\\n‚úÖ Seleccionando top {k} features originales basadas en loadings de PCA\")\n",
    "\n",
    "selected_features_pca = feature_importance_from_pca.head(k).index.tolist()\n",
    "\n",
    "print(f\"\\nüìã Features seleccionadas ({k}):\")\n",
    "for i, feat in enumerate(selected_features_pca, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# ========== PREPARAR DATASET CON FEATURES SELECCIONADAS ==========\n",
    "X_pca_selected = X_scaled[:, X.columns.isin(selected_features_pca)]\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset con features seleccionadas por PCA:\")\n",
    "print(f\"   Shape: {X_pca_selected.shape}\")\n",
    "print(f\"   Reducci√≥n: {X.shape[1]} ‚Üí {X_pca_selected.shape[1]} features\")\n",
    "\n",
    "# ========== VISUALIZAR COMPARACI√ìN ==========\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top 20 features por importancia PCA\n",
    "ax1.barh(range(20), feature_importance_from_pca.head(20).values, color='steelblue', alpha=0.7)\n",
    "ax1.set_yticks(range(20))\n",
    "ax1.set_yticklabels(feature_importance_from_pca.head(20).index, fontsize=9)\n",
    "ax1.set_xlabel('Importancia Total (Suma de Loadings Absolutos)', fontsize=11)\n",
    "ax1.set_title('Top 20 Features por Importancia en PCA\\n(Features originales con mayor peso en componentes)', fontsize=12)\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Distribuci√≥n de importancias\n",
    "ax2.hist(feature_importance_from_pca, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax2.set_xlabel('Importancia', fontsize=11)\n",
    "ax2.set_ylabel('Frecuencia', fontsize=11)\n",
    "ax2.set_title('Distribuci√≥n de Importancia de Features', fontsize=12)\n",
    "ax2.axvline(feature_importance_from_pca.iloc[k-1], color='red', linestyle='--', \n",
    "            label=f'Umbral (top {k})', linewidth=2)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-pca_loadings_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nüí° INTERPRETACI√ìN:\")\n",
    "print(\"   Estas features originales son las que 'explican' los componentes principales\")\n",
    "print(\"   Ventaja: Mantienen interpretabilidad (puedes decir 'GrLivArea importa')\")\n",
    "print(\"   Diferencia con PCA: Usas features originales, no combinaciones lineales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c330895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUACI√ìN DE PERFORMANCE: PCA vs ORIGINAL ===\n",
      "‚è±Ô∏è Esto puede tomar 1-2 minutos (cross-validation con 80 features)...\n",
      "\n",
      "üîÑ Evaluando modelo con features originales...\n",
      "\n",
      "‚úÖ BASELINE - Features Originales (81 features):\n",
      "   RMSE: $26,342 ¬± $4,353\n",
      "   R¬≤:   0.8885 ¬± 0.0311\n",
      "   Scores RMSE: ['$27,351', '$20,403', '$27,453', '$33,249', '$23,254']\n",
      "\n",
      "üîÑ Evaluando modelo con PCA (38 componentes)...\n",
      "‚úÖ PCA transformado: 81 ‚Üí 38 features\n",
      "\n",
      "‚úÖ PCA - Componentes Reducidos (38 componentes):\n",
      "   RMSE: $26,620 ¬± $4,082\n",
      "   R¬≤:   0.8859 ¬± 0.0317\n",
      "   Scores RMSE: ['$25,613', '$22,127', '$27,048', '$34,109', '$24,202']\n",
      "\n",
      "üîÑ Evaluando modelo con features originales seleccionadas por PCA loadings...\n",
      "\n",
      "‚úÖ PCA Loadings - Features Originales Seleccionadas (38 features):\n",
      "   RMSE: $27,020 ¬± $4,051\n",
      "   R¬≤:   0.8830 ¬± 0.0295\n",
      "   Scores RMSE: ['$27,912', '$21,642', '$28,373', '$33,361', '$23,812']\n",
      "\n",
      "================================================================================\n",
      "                  COMPARACI√ìN: ORIGINAL vs PCA vs PCA LOADINGS                  \n",
      "================================================================================\n",
      "\n",
      "üìä REDUCCI√ìN DIMENSIONAL:\n",
      "   Original: 81 features\n",
      "   PCA: 81 ‚Üí 38 componentes (53.1% reducci√≥n)\n",
      "   PCA Loadings: 81 ‚Üí 38 features originales (53.1% reducci√≥n)\n",
      "   Varianza retenida (PCA): 79.5%\n",
      "\n",
      "üìä PERFORMANCE COMPARATIVO:\n",
      "\n",
      "   M√©todo                               RMSE         R¬≤   Features\n",
      "   ------------------------- --------------- ---------- ----------\n",
      "   Original                  $        26,342     0.8885         81\n",
      "   PCA Componentes           $        26,620     0.8859         38\n",
      "   PCA Loadings (Originales) $        27,020     0.8830         38\n",
      "\n",
      "üìä DIFERENCIAS VS ORIGINAL:\n",
      "   PCA Componentes:  RMSE +278 (+1.1%) | R¬≤ -0.0026\n",
      "   PCA Loadings:     RMSE +678 (+2.6%) | R¬≤ -0.0055\n",
      "\n",
      "üí° INTERPRETACI√ìN:\n",
      "\n",
      "   üîµ PCA Componentes (PC1, PC2...):\n",
      "      ‚úÖ Mantiene performance similar con 53% reducci√≥n\n",
      "      ‚ö†Ô∏è Pero: Componentes son combinaciones lineales (menos interpretables)\n",
      "\n",
      "   üü¢ PCA Loadings (Features originales):\n",
      "      ‚úÖ Mantiene performance similar con 53% reducci√≥n\n",
      "      ‚úÖ Plus: Usa features originales (interpretables)\n",
      "\n",
      "   üíº PARA NEGOCIO:\n",
      "      - PCA Componentes: Mejor para modelos 'black box' donde solo importa precisi√≥n\n",
      "      - PCA Loadings: Mejor para negocio (puedes decir 'GrLivArea es importante')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor  # ‚ö†Ô∏è REGRESSOR, no Classifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suprimir warnings de convergencia\n",
    "\n",
    "# ========== MODELO BASELINE: TODAS LAS FEATURES ORIGINALES ==========\n",
    "print(\"\\n=== EVALUACI√ìN DE PERFORMANCE: PCA vs ORIGINAL ===\")\n",
    "print(\"‚è±Ô∏è Esto puede tomar 1-2 minutos (cross-validation con 80 features)...\\n\")\n",
    "\n",
    "print(\"üîÑ Evaluando modelo con features originales...\")\n",
    "rf_original = RandomForestRegressor(\n",
    "    random_state=42, \n",
    "    n_estimators=2000,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1             # Usar todos los cores\n",
    ")\n",
    "\n",
    "# Usar neg_mean_squared_error y neg_mean_absolute_error para CV\n",
    "scores_mse_original = -cross_val_score(rf_original, X_scaled, y, cv=5, \n",
    "                                        scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_original = cross_val_score(rf_original, X_scaled, y, cv=5, \n",
    "                                     scoring='r2', n_jobs=-1)\n",
    "\n",
    "rmse_original = np.sqrt(scores_mse_original)\n",
    "\n",
    "print(f\"\\n‚úÖ BASELINE - Features Originales ({X.shape[1]} features):\")\n",
    "print(f\"   RMSE: ${rmse_original.mean():,.0f} ¬± ${rmse_original.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_original.mean():.4f} ¬± {scores_r2_original.std():.4f}\")\n",
    "print(f\"   Scores RMSE: {[f'${x:,.0f}' for x in rmse_original]}\")\n",
    "\n",
    "# ========== MODELO CON PCA (80% VARIANZA) ==========\n",
    "print(f\"\\nüîÑ Evaluando modelo con PCA ({n_components_80} componentes)...\")\n",
    "\n",
    "pca_reduced = PCA(n_components= n_components_80)\n",
    "X_pca_reduced = pca_reduced.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"‚úÖ PCA transformado: {X.shape[1]} ‚Üí {X_pca_reduced.shape[1]} features\")\n",
    "\n",
    "# Evaluar con PCA\n",
    "rf_pca = RandomForestRegressor(\n",
    "    random_state=42, \n",
    "    n_estimators=100, \n",
    "    max_depth=15,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "scores_mse_pca = -cross_val_score(rf_pca, X_pca_reduced, y, cv=5, \n",
    "                                   scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_pca = cross_val_score(rf_pca, X_pca_reduced, y, cv=5, \n",
    "                                scoring='r2', n_jobs=-1)\n",
    "\n",
    "rmse_pca = np.sqrt(scores_mse_pca)\n",
    "\n",
    "print(f\"\\n‚úÖ PCA - Componentes Reducidos ({n_components_80} componentes):\")\n",
    "print(f\"   RMSE: ${rmse_pca.mean():,.0f} ¬± ${rmse_pca.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_pca.mean():.4f} ¬± {scores_r2_pca.std():.4f}\")\n",
    "print(f\"   Scores RMSE: {[f'${x:,.0f}' for x in rmse_pca]}\")\n",
    "\n",
    "# ========== MODELO CON FEATURES SELECCIONADAS POR PCA LOADINGS ==========\n",
    "print(f\"\\nüîÑ Evaluando modelo con features originales seleccionadas por PCA loadings...\")\n",
    "\n",
    "rf_pca_selected = RandomForestRegressor(\n",
    "    random_state=42, \n",
    "    n_estimators=100, \n",
    "    max_depth=15,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "scores_mse_pca_selected = -cross_val_score(rf_pca_selected, X_pca_selected, y, cv=5, \n",
    "                                             scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_pca_selected = cross_val_score(rf_pca_selected, X_pca_selected, y, cv=5, \n",
    "                                          scoring='r2', n_jobs=-1)\n",
    "\n",
    "rmse_pca_selected = np.sqrt(scores_mse_pca_selected)\n",
    "\n",
    "print(f\"\\n‚úÖ PCA Loadings - Features Originales Seleccionadas ({len(selected_features_pca)} features):\")\n",
    "print(f\"   RMSE: ${rmse_pca_selected.mean():,.0f} ¬± ${rmse_pca_selected.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_pca_selected.mean():.4f} ¬± {scores_r2_pca_selected.std():.4f}\")\n",
    "print(f\"   Scores RMSE: {[f'${x:,.0f}' for x in rmse_pca_selected]}\")\n",
    "\n",
    "# ========== AN√ÅLISIS COMPARATIVO ==========\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"{'COMPARACI√ìN: ORIGINAL vs PCA vs PCA LOADINGS':^80}\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "reduction_pct = (1 - n_components_80 / X.shape[1]) * 100\n",
    "rmse_diff_pca = rmse_pca.mean() - rmse_original.mean()\n",
    "rmse_diff_pca_selected = rmse_pca_selected.mean() - rmse_original.mean()\n",
    "r2_diff_pca = scores_r2_pca.mean() - scores_r2_original.mean()\n",
    "r2_diff_pca_selected = scores_r2_pca_selected.mean() - scores_r2_original.mean()\n",
    "\n",
    "print(f\"\\nüìä REDUCCI√ìN DIMENSIONAL:\")\n",
    "print(f\"   Original: {X.shape[1]} features\")\n",
    "print(f\"   PCA: {X.shape[1]} ‚Üí {n_components_80} componentes ({reduction_pct:.1f}% reducci√≥n)\")\n",
    "print(f\"   PCA Loadings: {X.shape[1]} ‚Üí {len(selected_features_pca)} features originales ({reduction_pct:.1f}% reducci√≥n)\")\n",
    "print(f\"   Varianza retenida (PCA): {pca_reduced.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE COMPARATIVO:\")\n",
    "print(f\"\\n   {'M√©todo':<25s} {'RMSE':>15s} {'R¬≤':>10s} {'Features':>10s}\")\n",
    "print(f\"   {'-'*25} {'-'*15} {'-'*10} {'-'*10}\")\n",
    "print(f\"   {'Original':<25s} ${rmse_original.mean():>14,.0f} {scores_r2_original.mean():>10.4f} {X.shape[1]:>10d}\")\n",
    "print(f\"   {'PCA Componentes':<25s} ${rmse_pca.mean():>14,.0f} {scores_r2_pca.mean():>10.4f} {n_components_80:>10d}\")\n",
    "print(f\"   {'PCA Loadings (Originales)':<25s} ${rmse_pca_selected.mean():>14,.0f} {scores_r2_pca_selected.mean():>10.4f} {len(selected_features_pca):>10d}\")\n",
    "\n",
    "print(f\"\\nüìä DIFERENCIAS VS ORIGINAL:\")\n",
    "print(f\"   PCA Componentes:  RMSE {rmse_diff_pca:+,.0f} ({(rmse_diff_pca/rmse_original.mean())*100:+.1f}%) | R¬≤ {r2_diff_pca:+.4f}\")\n",
    "print(f\"   PCA Loadings:     RMSE {rmse_diff_pca_selected:+,.0f} ({(rmse_diff_pca_selected/rmse_original.mean())*100:+.1f}%) | R¬≤ {r2_diff_pca_selected:+.4f}\")\n",
    "\n",
    "# Interpretaci√≥n\n",
    "print(f\"\\nüí° INTERPRETACI√ìN:\")\n",
    "print(f\"\\n   üîµ PCA Componentes (PC1, PC2...):\")\n",
    "if rmse_pca.mean() < rmse_original.mean() * 1.05:\n",
    "    print(f\"      ‚úÖ Mantiene performance similar con {reduction_pct:.0f}% reducci√≥n\")\n",
    "    print(f\"      ‚ö†Ô∏è Pero: Componentes son combinaciones lineales (menos interpretables)\")\n",
    "else:\n",
    "    print(f\"      ‚ö†Ô∏è Pierde precisi√≥n significativa ({(rmse_diff_pca/rmse_original.mean())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   üü¢ PCA Loadings (Features originales):\")\n",
    "if rmse_pca_selected.mean() < rmse_original.mean() * 1.05:\n",
    "    print(f\"      ‚úÖ Mantiene performance similar con {reduction_pct:.0f}% reducci√≥n\")\n",
    "    print(f\"      ‚úÖ Plus: Usa features originales (interpretables)\")\n",
    "else:\n",
    "    print(f\"      ‚ö†Ô∏è Pierde precisi√≥n ({(rmse_diff_pca_selected/rmse_original.mean())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   üíº PARA NEGOCIO:\")\n",
    "print(f\"      - PCA Componentes: Mejor para modelos 'black box' donde solo importa precisi√≥n\")\n",
    "print(f\"      - PCA Loadings: Mejor para negocio (puedes decir 'GrLivArea es importante')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15ff0982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILTER METHOD: F-TEST (ANOVA) ===\n",
      "F-test mide la relaci√≥n lineal entre cada feature y el target (SalePrice)\n",
      "\n",
      "Seleccionando top 38 features con F-test...\n",
      "\n",
      "‚úÖ Features seleccionadas por F-test (38):\n",
      "   1. PID\n",
      "   2. Lot Frontage\n",
      "   3. Lot Area\n",
      "   4. Lot Shape\n",
      "   5. Neighborhood\n",
      "   6. Overall Qual\n",
      "   7. Year Built\n",
      "   8. Year Remod/Add\n",
      "   9. Roof Style\n",
      "  10. Mas Vnr Type\n",
      "  11. Mas Vnr Area\n",
      "  12. Exter Qual\n",
      "  13. Foundation\n",
      "  14. Bsmt Qual\n",
      "  15. Bsmt Exposure\n",
      "  16. BsmtFin SF 1\n",
      "  17. Total Bsmt SF\n",
      "  18. Heating QC\n",
      "  19. Central Air\n",
      "  20. Electrical\n",
      "  21. 1st Flr SF\n",
      "  22. 2nd Flr SF\n",
      "  23. Gr Liv Area\n",
      "  24. Bsmt Full Bath\n",
      "  25. Full Bath\n",
      "  26. Half Bath\n",
      "  27. Kitchen Qual\n",
      "  28. TotRms AbvGrd\n",
      "  29. Fireplaces\n",
      "  30. Garage Type\n",
      "  31. Garage Yr Blt\n",
      "  32. Garage Finish\n",
      "  33. Garage Cars\n",
      "  34. Garage Area\n",
      "  35. Paved Drive\n",
      "  36. Wood Deck SF\n",
      "  37. Open Porch SF\n",
      "  38. Sale Condition\n",
      "\n",
      "=== TOP 15 F-SCORES (Mayor correlaci√≥n con SalePrice) ===\n",
      "   1. Overall Qual        : 5,179\n",
      "   2. Gr Liv Area         : 2,923\n",
      "   3. Garage Cars         : 2,117\n",
      "   4. Exter Qual          : 2,115\n",
      "   5. Garage Area         : 2,035\n",
      "   6. Total Bsmt SF       : 1,949\n",
      "   7. 1st Flr SF          : 1,844\n",
      "   8. Bsmt Qual           : 1,807\n",
      "   9. Kitchen Qual        : 1,769\n",
      "  10. Year Built          : 1,327\n",
      "  11. Full Bath           : 1,241\n",
      "  12. Garage Finish       : 1,208\n",
      "  13. Year Remod/Add      : 1,162\n",
      "  14. Garage Yr Blt       : 1,023\n",
      "  15. Mas Vnr Area        : 987\n",
      "\n",
      "üîÑ Evaluando F-test...\n",
      "‚úÖ F-test (38 features):\n",
      "   RMSE: $26,395 ¬± $4,083\n",
      "   R¬≤:   0.8883 ¬± 0.0289\n",
      "\n",
      "=== FILTER METHOD: MUTUAL INFORMATION ===\n",
      "Mutual Information mide dependencia no-lineal entre features y target\n",
      "\n",
      "Seleccionando top 38 features con Mutual Information...\n",
      "\n",
      "‚úÖ Features seleccionadas por MI (38):\n",
      "   1. Order\n",
      "   2. PID\n",
      "   3. MS SubClass\n",
      "   4. MS Zoning\n",
      "   5. Lot Frontage\n",
      "   6. Lot Area\n",
      "   7. Neighborhood\n",
      "   8. Overall Qual\n",
      "   9. Overall Cond\n",
      "  10. Year Built\n",
      "  11. Year Remod/Add\n",
      "  12. Exterior 1st\n",
      "  13. Exterior 2nd\n",
      "  14. Mas Vnr Area\n",
      "  15. Exter Qual\n",
      "  16. Foundation\n",
      "  17. Bsmt Qual\n",
      "  18. BsmtFin Type 1\n",
      "  19. BsmtFin SF 1\n",
      "  20. Bsmt Unf SF\n",
      "  21. Total Bsmt SF\n",
      "  22. Heating QC\n",
      "  23. 1st Flr SF\n",
      "  24. 2nd Flr SF\n",
      "  25. Gr Liv Area\n",
      "  26. Full Bath\n",
      "  27. Kitchen Qual\n",
      "  28. TotRms AbvGrd\n",
      "  29. Fireplaces\n",
      "  30. Fireplace Qu\n",
      "  31. Garage Type\n",
      "  32. Garage Yr Blt\n",
      "  33. Garage Finish\n",
      "  34. Garage Cars\n",
      "  35. Garage Area\n",
      "  36. Wood Deck SF\n",
      "  37. Open Porch SF\n",
      "  38. Sale Type\n",
      "\n",
      "=== TOP 15 MI SCORES ===\n",
      "   1. Overall Qual        : 0.5856\n",
      "   2. Neighborhood        : 0.5623\n",
      "   3. Gr Liv Area         : 0.4962\n",
      "   4. Garage Area         : 0.4177\n",
      "   5. Total Bsmt SF       : 0.3929\n",
      "   6. Year Built          : 0.3890\n",
      "   7. Garage Cars         : 0.3876\n",
      "   8. 1st Flr SF          : 0.3707\n",
      "   9. Bsmt Qual           : 0.3479\n",
      "  10. Garage Yr Blt       : 0.3289\n",
      "  11. Exter Qual          : 0.3251\n",
      "  12. Kitchen Qual        : 0.3201\n",
      "  13. Year Remod/Add      : 0.2939\n",
      "  14. MS SubClass         : 0.2848\n",
      "  15. Order               : 0.2815\n",
      "\n",
      "üîÑ Evaluando Mutual Information...\n",
      "‚úÖ MI (38 features):\n",
      "   RMSE: $26,279 ¬± $4,281\n",
      "   R¬≤:   0.8891 ¬± 0.0305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "# ========== F-TEST PARA REGRESI√ìN ==========\n",
    "print(\"\\n=== FILTER METHOD: F-TEST (ANOVA) ===\")\n",
    "print(\"F-test mide la relaci√≥n lineal entre cada feature y el target (SalePrice)\")\n",
    "\n",
    "k = n_components_80  # Mismo n√∫mero que PCA para comparaci√≥n justa\n",
    "\n",
    "print(f\"\\nSeleccionando top {k} features con F-test...\")\n",
    "\n",
    "selector_f = SelectKBest(score_func=f_regression, k=k)\n",
    "X_filter_f = selector_f.fit_transform(X_scaled, y)\n",
    "\n",
    "# Identificar features seleccionadas\n",
    "selected_features_f = X.columns[selector_f.get_support()]\n",
    "print(f\"\\n‚úÖ Features seleccionadas por F-test ({k}):\")\n",
    "for i, feat in enumerate(selected_features_f, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# ========== SCORES DE F-TEST ==========\n",
    "scores_f = pd.Series(selector_f.scores_, index=X.columns).sort_values(ascending=False)\n",
    "print(f\"\\n=== TOP 15 F-SCORES (Mayor correlaci√≥n con SalePrice) ===\")\n",
    "for i, (feat, score) in enumerate(scores_f.head(15).items(), 1):\n",
    "    print(f\"  {i:2d}. {feat:20s}: {score:,.0f}\")\n",
    "\n",
    "# Visualizar scores (top 30 para claridad)\n",
    "plt.figure(figsize=(14, 10))\n",
    "scores_f.head(30).sort_values(ascending=True).plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('F-Score (ANOVA)', fontsize=12)\n",
    "plt.title('Top 30 Features por F-test\\n(Mayor F-score = Mayor relaci√≥n lineal con SalePrice)', fontsize=14)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-f_test_scores.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ========== EVALUAR F-TEST ==========\n",
    "print(f\"\\nüîÑ Evaluando F-test...\")\n",
    "rf_filter_f = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=15, n_jobs=-1)\n",
    "scores_mse_filter_f = -cross_val_score(rf_filter_f, X_filter_f, y, cv=5, \n",
    "                                        scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_filter_f = cross_val_score(rf_filter_f, X_filter_f, y, cv=5, \n",
    "                                      scoring='r2', n_jobs=-1)\n",
    "rmse_filter_f = np.sqrt(scores_mse_filter_f)\n",
    "\n",
    "print(f\"‚úÖ F-test ({k} features):\")\n",
    "print(f\"   RMSE: ${rmse_filter_f.mean():,.0f} ¬± ${rmse_filter_f.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_filter_f.mean():.4f} ¬± {scores_r2_filter_f.std():.4f}\")\n",
    "\n",
    "# ========== MUTUAL INFORMATION ==========\n",
    "print(\"\\n=== FILTER METHOD: MUTUAL INFORMATION ===\")\n",
    "print(\"Mutual Information mide dependencia no-lineal entre features y target\")\n",
    "\n",
    "print(f\"\\nSeleccionando top {k} features con Mutual Information...\")\n",
    "\n",
    "selector_mi = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "X_filter_mi = selector_mi.fit_transform(X_scaled, y)\n",
    "\n",
    "selected_features_mi = X.columns[selector_mi.get_support()]\n",
    "print(f\"\\n‚úÖ Features seleccionadas por MI ({k}):\")\n",
    "for i, feat in enumerate(selected_features_mi, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "scores_mi = pd.Series(selector_mi.scores_, index=X.columns).sort_values(ascending=False)\n",
    "print(f\"\\n=== TOP 15 MI SCORES ===\")\n",
    "for i, (feat, score) in enumerate(scores_mi.head(15).items(), 1):\n",
    "    print(f\"  {i:2d}. {feat:20s}: {score:.4f}\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(14, 10))\n",
    "scores_mi.head(30).sort_values(ascending=True).plot(kind='barh', color='darkgreen')\n",
    "plt.xlabel('Mutual Information', fontsize=12)\n",
    "plt.title('Top 30 Features por Mutual Information\\n(Captura dependencias no-lineales)', fontsize=14)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-mi_scores.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Evaluar\n",
    "print(f\"\\nüîÑ Evaluando Mutual Information...\")\n",
    "rf_filter_mi = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=15, n_jobs=-1)\n",
    "scores_mse_filter_mi = -cross_val_score(rf_filter_mi, X_filter_mi, y, cv=5, \n",
    "                                         scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_filter_mi = cross_val_score(rf_filter_mi, X_filter_mi, y, cv=5, \n",
    "                                       scoring='r2', n_jobs=-1)\n",
    "rmse_filter_mi = np.sqrt(scores_mse_filter_mi)\n",
    "\n",
    "print(f\"‚úÖ MI ({k} features):\")\n",
    "print(f\"   RMSE: ${rmse_filter_mi.mean():,.0f} ¬± ${rmse_filter_mi.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_filter_mi.mean():.4f} ¬± {scores_r2_filter_mi.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19802548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WRAPPER METHOD: FORWARD SELECTION ===\n",
      "üí° ESTRATEGIA: Forward Selection sobre features pre-seleccionadas por PCA\n",
      "   Stage 1 (ya hecho): PCA Loadings ‚Üí 38 features\n",
      "   Stage 2 (ahora): Forward Selection ‚Üí refinar a menos features\n",
      "‚è±Ô∏è Esto tomar√° ~30-60 segundos (mucho m√°s r√°pido que sobre 79 features)...\n",
      "\n",
      "üéØ Target: Seleccionar 19 features con Forward Selection\n",
      "üîÑ Ejecutando Forward Selection sobre 38 features pre-seleccionadas...\n",
      "‚úÖ Forward Selection completado en 499.6 segundos\n",
      "\n",
      "‚úÖ Features seleccionadas por Forward Selection (19):\n",
      "   1. TotRms AbvGrd\n",
      "   2. 2nd Flr SF\n",
      "   3. BsmtFin SF 1\n",
      "   4. Full Bath\n",
      "   5. BsmtFin Type 1\n",
      "   6. Half Bath\n",
      "   7. Overall Qual\n",
      "   8. Bsmt Exposure\n",
      "   9. Garage Yr Blt\n",
      "  10. Paved Drive\n",
      "  11. Bsmt Qual\n",
      "  12. Exter Qual\n",
      "  13. Kitchen Qual\n",
      "  14. Garage Area\n",
      "  15. Heating QC\n",
      "  16. Central Air\n",
      "  17. Electrical\n",
      "  18. Wood Deck SF\n",
      "  19. BsmtFin Type 2\n",
      "\n",
      "=== WRAPPER METHOD: BACKWARD ELIMINATION ===\n",
      "üí° ESTRATEGIA: Backward Elimination sobre features pre-seleccionadas por PCA\n",
      "   Stage 1 (ya hecho): PCA Loadings ‚Üí 38 features\n",
      "   Stage 2 (ahora): Backward Elimination ‚Üí refinar a 19 features\n",
      "üîÑ Ejecutando Backward Elimination sobre 38 features...\n",
      "‚úÖ Backward Elimination completado en 1154.5 segundos\n",
      "\n",
      "‚úÖ Features seleccionadas por Backward Elimination (19):\n",
      "   1. TotRms AbvGrd\n",
      "   2. 2nd Flr SF\n",
      "   3. BsmtFin SF 1\n",
      "   4. Full Bath\n",
      "   5. Year Built\n",
      "   6. Bedroom AbvGr\n",
      "   7. Overall Qual\n",
      "   8. Bsmt Exposure\n",
      "   9. Garage Type\n",
      "  10. Garage Yr Blt\n",
      "  11. Garage Finish\n",
      "  12. Bsmt Qual\n",
      "  13. Year Remod/Add\n",
      "  14. Central Air\n",
      "  15. Open Porch SF\n",
      "  16. Garage Qual\n",
      "  17. Foundation\n",
      "  18. Enclosed Porch\n",
      "  19. BsmtFin Type 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# ========== TWO-STAGE SELECTION: PCA + FORWARD ==========\n",
    "print(\"\\n=== WRAPPER METHOD: FORWARD SELECTION ===\")\n",
    "print(\"üí° ESTRATEGIA: Forward Selection sobre features pre-seleccionadas por PCA\")\n",
    "print(f\"   Stage 1 (ya hecho): PCA Loadings ‚Üí {len(selected_features_pca)} features\")\n",
    "print(f\"   Stage 2 (ahora): Forward Selection ‚Üí refinar a menos features\")\n",
    "print(\"‚è±Ô∏è Esto tomar√° ~30-60 segundos (mucho m√°s r√°pido que sobre 79 features)...\\n\")\n",
    "\n",
    "# Decidir cu√°ntas features seleccionar con wrapper\n",
    "k_wrapper = max(15, k // 2)  # Aproximadamente la mitad de las features PCA, o m√≠nimo 15\n",
    "print(f\"üéØ Target: Seleccionar {k_wrapper} features con Forward Selection\")\n",
    "\n",
    "estimator_forward = RandomForestRegressor(\n",
    "    random_state=42, \n",
    "    n_estimators= 20,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "selector_forward = SequentialFeatureSelector(\n",
    "    estimator=estimator_forward, \n",
    "    n_features_to_select=k_wrapper,\n",
    "    direction= 'forward',\n",
    "    cv=3,              # 3 folds para rapidez\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"üîÑ Ejecutando Forward Selection sobre {len(selected_features_pca)} features pre-seleccionadas...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "X_forward = selector_forward.fit_transform(X_pca_selected, y)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Forward Selection completado en {elapsed_time:.1f} segundos\")\n",
    "\n",
    "# Features seleccionadas (mapear √≠ndices de vuelta a nombres)\n",
    "selected_indices_forward = selector_forward.get_support()\n",
    "selected_features_forward = [selected_features_pca[i] for i, sel in enumerate(selected_indices_forward) if sel]\n",
    "\n",
    "print(f\"\\n‚úÖ Features seleccionadas por Forward Selection ({len(selected_features_forward)}):\")\n",
    "for i, feat in enumerate(selected_features_forward, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# ========== BACKWARD ELIMINATION ==========\n",
    "print(\"\\n=== WRAPPER METHOD: BACKWARD ELIMINATION ===\")\n",
    "print(\"üí° ESTRATEGIA: Backward Elimination sobre features pre-seleccionadas por PCA\")\n",
    "print(f\"   Stage 1 (ya hecho): PCA Loadings ‚Üí {len(selected_features_pca)} features\")\n",
    "print(f\"   Stage 2 (ahora): Backward Elimination ‚Üí refinar a {k_wrapper} features\")\n",
    "\n",
    "estimator_backward = RandomForestRegressor(\n",
    "    random_state=42, \n",
    "    n_estimators=20,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "selector_backward = SequentialFeatureSelector(\n",
    "    estimator=estimator_backward, \n",
    "    n_features_to_select=k_wrapper,\n",
    "    direction='backward',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"üîÑ Ejecutando Backward Elimination sobre {len(selected_features_pca)} features...\")\n",
    "start_time = time.time()\n",
    "X_backward = selector_backward.fit_transform(X_pca_selected, y)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Backward Elimination completado en {elapsed_time:.1f} segundos\")\n",
    "\n",
    "selected_indices_backward = selector_backward.get_support()\n",
    "selected_features_backward = [selected_features_pca[i] for i, sel in enumerate(selected_indices_backward) if sel]\n",
    "\n",
    "print(f\"\\n‚úÖ Features seleccionadas por Backward Elimination ({len(selected_features_backward)}):\")\n",
    "for i, feat in enumerate(selected_features_backward, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4ea4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WRAPPER METHOD: RFE (Recursive Feature Elimination) ===\n",
      "üí° ESTRATEGIA: RFE sobre features pre-seleccionadas por PCA\n",
      "   Stage 1 (ya hecho): PCA Loadings ‚Üí 38 features\n",
      "   Stage 2 (ahora): RFE ‚Üí refinar a 19 features\n",
      "‚è±Ô∏è Esto tomar√° ~45-90 segundos...\n",
      "\n",
      "üîÑ Ejecutando RFE sobre 38 features...\n",
      "‚úÖ RFE completado en 22.1 segundos\n",
      "\n",
      "‚úÖ Features seleccionadas por RFE (19):\n",
      "   1. TotRms AbvGrd\n",
      "   2. 2nd Flr SF\n",
      "   3. BsmtFin SF 1\n",
      "   4. Bsmt Full Bath\n",
      "   5. Total Bsmt SF\n",
      "   6. Overall Qual\n",
      "   7. 1st Flr SF\n",
      "   8. Bsmt Exposure\n",
      "   9. Garage Finish\n",
      "  10. Paved Drive\n",
      "  11. Bsmt Qual\n",
      "  12. Kitchen Qual\n",
      "  13. House Style\n",
      "  14. Garage Qual\n",
      "  15. Foundation\n",
      "  16. Electrical\n",
      "  17. Kitchen AbvGr\n",
      "  18. Mas Vnr Area\n",
      "  19. BsmtFin Type 2\n",
      "\n",
      "Ranking de features (1 = seleccionada, solo mostrando top 20):\n",
      "TotRms AbvGrd     1\n",
      "2nd Flr SF        1\n",
      "BsmtFin SF 1      1\n",
      "Bsmt Full Bath    1\n",
      "1st Flr SF        1\n",
      "Bsmt Exposure     1\n",
      "Overall Qual      1\n",
      "Total Bsmt SF     1\n",
      "House Style       1\n",
      "Electrical        1\n",
      "Foundation        1\n",
      "Garage Qual       1\n",
      "Bsmt Qual         1\n",
      "Kitchen Qual      1\n",
      "Garage Finish     1\n",
      "Paved Drive       1\n",
      "Mas Vnr Area      1\n",
      "BsmtFin Type 2    1\n",
      "Kitchen AbvGr     1\n",
      "Full Bath         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# ========== TWO-STAGE SELECTION: PCA + RFE ==========\n",
    "print(\"\\n=== WRAPPER METHOD: RFE (Recursive Feature Elimination) ===\")\n",
    "print(\"üí° ESTRATEGIA: RFE sobre features pre-seleccionadas por PCA\")\n",
    "print(f\"   Stage 1 (ya hecho): PCA Loadings ‚Üí {len(selected_features_pca)} features\")\n",
    "print(f\"   Stage 2 (ahora): RFE ‚Üí refinar a {k_wrapper} features\")\n",
    "print(\"‚è±Ô∏è Esto tomar√° ~45-90 segundos...\\n\")\n",
    "\n",
    "estimator = RandomForestRegressor(\n",
    "    random_state=42, \n",
    "    n_estimators=20,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1\n",
    ")\n",
    "selector_rfe = RFE(estimator=estimator, n_features_to_select=k_wrapper, step=1)\n",
    "\n",
    "print(f\"üîÑ Ejecutando RFE sobre {len(selected_features_pca)} features...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "X_rfe = selector_rfe.fit_transform(X_pca_selected, y)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ RFE completado en {elapsed_time:.1f} segundos\")\n",
    "\n",
    "# Features seleccionadas (mapear √≠ndices de vuelta a nombres)\n",
    "selected_indices_rfe = selector_rfe.get_support()\n",
    "selected_features_rfe = [selected_features_pca[i] for i, sel in enumerate(selected_indices_rfe) if sel]\n",
    "\n",
    "print(f\"\\n‚úÖ Features seleccionadas por RFE ({len(selected_features_rfe)}):\")\n",
    "for i, feat in enumerate(selected_features_rfe, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Ranking de features (solo sobre las pre-seleccionadas por PCA)\n",
    "ranking = pd.Series(selector_rfe.ranking_, index=selected_features_pca).sort_values()\n",
    "print(f\"\\nRanking de features (1 = seleccionada, solo mostrando top 20):\")\n",
    "print(ranking.head(20))\n",
    "\n",
    "# Visualizar ranking (top 30 para claridad)\n",
    "plt.figure(figsize=(12, 8))\n",
    "ranking.head(30).sort_values(ascending=False).plot(kind='barh')\n",
    "plt.xlabel('Ranking (1 = mejor, n√∫meros mayores = eliminadas antes)')\n",
    "plt.title(f'RFE Feature Ranking - Top 30 de {len(selected_features_pca)} features pre-seleccionadas')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-rfe_ranking.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9536126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARACI√ìN DE FEATURES SELECCIONADAS ===\n",
      "\n",
      "Features consistentes (en todos los m√©todos): 15\n",
      "['Heating QC', 'BsmtFin Type 2', 'Year Remod/Add', 'Central Air', 'Paved Drive', 'Garage Type', 'Exter Qual', 'Garage Area', 'Year Built', 'Electrical', '1st Flr SF', 'Open Porch SF', 'Total Bsmt SF', 'Mas Vnr Area', 'Wood Deck SF']\n",
      "\n",
      "Features robustas (‚â•2 m√©todos): 37\n",
      "['Full Bath', 'Heating QC', 'Lot Frontage', 'BsmtFin Type 2', 'Half Bath', 'Overall Qual', 'Year Remod/Add', 'Garage Qual', 'Foundation', 'Central Air', 'Paved Drive', 'Bsmt Full Bath', 'Garage Type', 'Exter Qual', 'TotRms AbvGrd', 'Bsmt Qual', 'Garage Area', 'Year Built', 'Electrical', '1st Flr SF', 'PID', 'Fireplaces', 'Open Porch SF', 'Kitchen Qual', 'Bsmt Exposure', 'BsmtFin SF 1', 'Garage Yr Blt', '2nd Flr SF', 'Total Bsmt SF', 'BsmtFin Type 1', 'Garage Finish', 'Mas Vnr Area', 'Neighborhood', 'Wood Deck SF', 'Garage Cars', 'Gr Liv Area', 'Lot Area']\n",
      "\n",
      "üí° OBSERVACI√ìN:\n",
      "   Forward, Backward y RFE son todos wrapper methods, pero usan estrategias diferentes\n"
     ]
    }
   ],
   "source": [
    "# Comparar features seleccionadas por diferentes m√©todos\n",
    "print(\"\\n=== COMPARACI√ìN DE FEATURES SELECCIONADAS ===\")\n",
    "\n",
    "# Crear conjunto de features por m√©todo\n",
    "features_dict = {\n",
    "    'F-test': set(selected_features_f),\n",
    "    'Mutual Info': set(selected_features_mi),\n",
    "    'Forward': set(selected_features_forward),\n",
    "    'Backward': set(selected_features_backward),\n",
    "    'RFE': set(selected_features_rfe)\n",
    "}\n",
    "\n",
    "# Features en al menos 2 m√©todos\n",
    "all_features = set()\n",
    "for features in features_dict.values():\n",
    "    all_features.update(features)\n",
    "\n",
    "feature_counts = {}\n",
    "for feature in all_features:\n",
    "    count = sum(1 for features in features_dict.values() if feature in features)\n",
    "    feature_counts[feature] = count\n",
    "\n",
    "# Features consistentes (en todos los m√©todos)\n",
    "consistent_features = [f for f, count in feature_counts.items() if count == 3]\n",
    "print(f\"\\nFeatures consistentes (en todos los m√©todos): {len(consistent_features)}\")\n",
    "print(consistent_features)\n",
    "\n",
    "# Features en al menos 2 m√©todos\n",
    "robust_features = [f for f, count in feature_counts.items() if count >= 2]\n",
    "print(f\"\\nFeatures robustas (‚â•2 m√©todos): {len(robust_features)}\")\n",
    "print(robust_features)\n",
    "\n",
    "print(\"\\nüí° OBSERVACI√ìN:\")\n",
    "print(f\"   Forward, Backward y RFE son todos wrapper methods, pero usan estrategias diferentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8786bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUANDO WRAPPER METHODS ===\n",
      "‚è±Ô∏è Cross-validation con features de cada m√©todo...\n",
      "\n",
      "üîÑ Evaluando Forward Selection...\n",
      "‚úÖ Forward Selection (19 features):\n",
      "   RMSE: $27,036 ¬± $3,795\n",
      "   R¬≤:   0.8828 ¬± 0.0283\n",
      "\n",
      "üîÑ Evaluando Backward Elimination...\n",
      "‚úÖ Backward Elimination (19 features):\n",
      "   RMSE: $27,084 ¬± $2,945\n",
      "   R¬≤:   0.8827 ¬± 0.0222\n",
      "\n",
      "üîÑ Evaluando RFE...\n",
      "‚úÖ RFE (19 features):\n",
      "   RMSE: $27,557 ¬± $4,051\n",
      "   R¬≤:   0.8782 ¬± 0.0304\n",
      "\n",
      "================================================================================\n",
      "                   COMPARACI√ìN: TODOS LOS M√âTODOS HASTA AHORA                   \n",
      "================================================================================\n",
      "\n",
      "         M√©todo  N_Features         RMSE       R¬≤  Reducci√≥n%\n",
      "             MI          38 26279.032615 0.889133    53.08642\n",
      "       Original          81 26341.942320 0.888494     0.00000\n",
      "         F-test          38 26394.575518 0.888328    53.08642\n",
      "PCA Componentes          38 26619.948441 0.885860    53.08642\n",
      "   PCA Loadings          38 27020.052801 0.882950    53.08642\n",
      "        Forward          38 27036.223643 0.882806    53.08642\n",
      "       Backward          38 27084.127714 0.882717    53.08642\n",
      "            RFE          38 27557.106392 0.878213    53.08642\n",
      "\n",
      "üí° OBSERVACI√ìN:\n",
      "   üèÜ Mejor RMSE: MI ($26,279)\n"
     ]
    }
   ],
   "source": [
    "# ========== EVALUACI√ìN: FORWARD SELECTION ==========\n",
    "print(\"\\n=== EVALUANDO WRAPPER METHODS ===\")\n",
    "print(\"‚è±Ô∏è Cross-validation con features de cada m√©todo...\\n\")\n",
    "\n",
    "print(\"üîÑ Evaluando Forward Selection...\")\n",
    "rf_forward = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=15, n_jobs=-1)\n",
    "\n",
    "scores_mse_forward = -cross_val_score(rf_forward, X_forward, y, cv=5, \n",
    "                                       scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_forward = cross_val_score(rf_forward, X_forward, y, cv=5, \n",
    "                                    scoring='r2', n_jobs=-1)\n",
    "rmse_forward = np.sqrt(scores_mse_forward)\n",
    "\n",
    "print(f\"‚úÖ Forward Selection ({len(selected_features_forward)} features):\")\n",
    "print(f\"   RMSE: ${rmse_forward.mean():,.0f} ¬± ${rmse_forward.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_forward.mean():.4f} ¬± {scores_r2_forward.std():.4f}\")\n",
    "\n",
    "# ========== EVALUACI√ìN: BACKWARD ELIMINATION ==========\n",
    "print(f\"\\nüîÑ Evaluando Backward Elimination...\")\n",
    "rf_backward = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=15, n_jobs=-1)\n",
    "\n",
    "scores_mse_backward = -cross_val_score(rf_backward, X_backward, y, cv=5, \n",
    "                                         scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_backward = cross_val_score(rf_backward, X_backward, y, cv=5, \n",
    "                                      scoring='r2', n_jobs=-1)\n",
    "rmse_backward = np.sqrt(scores_mse_backward)\n",
    "\n",
    "print(f\"‚úÖ Backward Elimination ({len(selected_features_backward)} features):\")\n",
    "print(f\"   RMSE: ${rmse_backward.mean():,.0f} ¬± ${rmse_backward.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_backward.mean():.4f} ¬± {scores_r2_backward.std():.4f}\")\n",
    "\n",
    "# ========== EVALUACI√ìN: RFE ==========\n",
    "print(f\"\\nüîÑ Evaluando RFE...\")\n",
    "rf_rfe = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=15, n_jobs=-1)\n",
    "\n",
    "scores_mse_rfe = -cross_val_score(rf_rfe, X_rfe, y, cv=5, \n",
    "                                   scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_rfe = cross_val_score(rf_rfe, X_rfe, y, cv=5, \n",
    "                                scoring='r2', n_jobs=-1)\n",
    "rmse_rfe = np.sqrt(scores_mse_rfe)\n",
    "\n",
    "print(f\"‚úÖ RFE ({len(selected_features_rfe)} features):\")\n",
    "print(f\"   RMSE: ${rmse_rfe.mean():,.0f} ¬± ${rmse_rfe.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_rfe.mean():.4f} ¬± {scores_r2_rfe.std():.4f}\")\n",
    "\n",
    "# ========== COMPARACI√ìN ACTUALIZADA ==========\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"{'COMPARACI√ìN: TODOS LOS M√âTODOS HASTA AHORA':^80}\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "comparison_updated = {\n",
    "    'M√©todo': ['Original', 'PCA Componentes', 'PCA Loadings', 'F-test', 'MI', 'Forward', 'Backward', 'RFE'],\n",
    "    'N_Features': [X.shape[1], n_components_80, k, k, k, k, k, k],\n",
    "    'RMSE': [rmse_original.mean(), rmse_pca.mean(), rmse_pca_selected.mean(), rmse_filter_f.mean(), \n",
    "             rmse_filter_mi.mean(), rmse_forward.mean(), rmse_backward.mean(), rmse_rfe.mean()],\n",
    "    'R¬≤': [scores_r2_original.mean(), scores_r2_pca.mean(), scores_r2_pca_selected.mean(), scores_r2_filter_f.mean(), \n",
    "           scores_r2_filter_mi.mean(), scores_r2_forward.mean(), scores_r2_backward.mean(), scores_r2_rfe.mean()]\n",
    "}\n",
    "comparison_updated_df = pd.DataFrame(comparison_updated)\n",
    "comparison_updated_df['Reducci√≥n%'] = (1 - comparison_updated_df['N_Features'] / X.shape[1]) * 100\n",
    "comparison_updated_df = comparison_updated_df.sort_values('RMSE')\n",
    "\n",
    "print(f\"\\n{comparison_updated_df.to_string(index=False)}\")\n",
    "\n",
    "print(f\"\\nüí° OBSERVACI√ìN:\")\n",
    "best_method = comparison_updated_df.iloc[0]['M√©todo']\n",
    "best_rmse = comparison_updated_df.iloc[0]['RMSE']\n",
    "print(f\"   üèÜ Mejor RMSE: {best_method} (${best_rmse:,.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63b6ea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EMBEDDED METHODS: Random Forest ===\n",
      "Top 10 features por importancia:\n",
      "Overall Qual     0.639275\n",
      "Gr Liv Area      0.109502\n",
      "Total Bsmt SF    0.033898\n",
      "1st Flr SF       0.032558\n",
      "BsmtFin SF 1     0.019934\n",
      "2nd Flr SF       0.015710\n",
      "Lot Area         0.013787\n",
      "Garage Area      0.010136\n",
      "Year Built       0.009509\n",
      "PID              0.008036\n",
      "dtype: float64\n",
      "\n",
      "Features seleccionadas por RF Importance (38):\n",
      "['Overall Qual', 'Gr Liv Area', 'Total Bsmt SF', '1st Flr SF', 'BsmtFin SF 1', '2nd Flr SF', 'Lot Area', 'Garage Area', 'Year Built', 'PID', 'Garage Cars', 'Year Remod/Add', 'Full Bath', 'Bsmt Qual', 'Lot Frontage', 'Bsmt Unf SF', 'Mas Vnr Area', 'Open Porch SF', 'Overall Cond', 'Kitchen Qual', 'Wood Deck SF', 'Garage Yr Blt', 'Neighborhood', 'Order', 'Mo Sold', 'Garage Finish', 'Fireplaces', 'MS Zoning', 'TotRms AbvGrd', 'Screen Porch', 'Sale Condition', 'Exter Qual', 'MS SubClass', 'Bsmt Full Bath', 'Bedroom AbvGr', 'Bsmt Exposure', 'BsmtFin Type 1', 'Central Air']\n",
      "\n",
      "‚úÖ RF Importance (38 features):\n",
      "   RMSE: $26,238 ¬± $4,514\n",
      "   R¬≤:   0.8894 ¬± 0.0318\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EMBEDDED METHODS: Random Forest ===\")\n",
    "\n",
    "rf_embedded = RandomForestRegressor(random_state=42, n_estimators=200, max_depth=15, n_jobs=-1)\n",
    "rf_embedded.fit(X_scaled, y)\n",
    "\n",
    "# Feature importances\n",
    "importances = pd.Series(rf_embedded.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Top 10 features por importancia:\")\n",
    "print(importances.head(10))\n",
    "\n",
    "# Visualizar importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "importances.sort_values(ascending=True).plot(kind='barh')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-rf_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Seleccionar top-k features\n",
    "top_k_features = importances.nlargest(k).index\n",
    "X_rf_importance = X_scaled[:, X.columns.isin(top_k_features)]\n",
    "\n",
    "print(f\"\\nFeatures seleccionadas por RF Importance ({k}):\")\n",
    "print(list(top_k_features))\n",
    "\n",
    "# Evaluar\n",
    "rmse_rf_importance = np.sqrt(-cross_val_score(rf_embedded, X_rf_importance, y, cv=5, \n",
    "                                                scoring='neg_mean_squared_error', n_jobs=-1))\n",
    "scores_r2_rf_importance = cross_val_score(rf_embedded, X_rf_importance, y, cv=5, \n",
    "                                          scoring='r2', n_jobs=-1)\n",
    "\n",
    "print(f\"\\n‚úÖ RF Importance ({k} features):\")\n",
    "print(f\"   RMSE: ${rmse_rf_importance.mean():,.0f} ¬± ${rmse_rf_importance.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_rf_importance.mean():.4f} ¬± {scores_r2_rf_importance.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4fd3c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EMBEDDED METHOD: Lasso (L1 Regularization) ===\n",
      "Lasso penaliza coeficientes, forzando a 0 features no importantes\n",
      "‚è±Ô∏è Esto puede tomar 30-60 segundos...\n",
      "\n",
      "‚úÖ Lasso alpha seleccionado: 1375.3800\n",
      "\n",
      "üìä Features con coeficiente no-cero: 41 de 81\n",
      "\n",
      "‚úÖ Top 38 features por magnitud de coeficiente Lasso:\n",
      "   1. Gr Liv Area         : |23965.928207|\n",
      "   2. Overall Qual        : |18865.442199|\n",
      "   3. Exter Qual          : |7716.360557|\n",
      "   4. Bsmt Qual           : |6329.475846|\n",
      "   5. BsmtFin SF 1        : |5992.855445|\n",
      "   6. Kitchen Qual        : |5700.002821|\n",
      "   7. MS SubClass         : |5239.648051|\n",
      "   8. Year Built          : |4413.604581|\n",
      "   9. Mas Vnr Area        : |3986.298254|\n",
      "  10. Fireplaces          : |3714.132817|\n",
      "  11. Garage Cars         : |3635.912839|\n",
      "  12. Misc Val            : |3352.852943|\n",
      "  13. Bsmt Exposure       : |3248.117435|\n",
      "  14. Overall Cond        : |3078.550394|\n",
      "  15. Garage Area         : |3068.732335|\n",
      "  16. Lot Area            : |2623.674245|\n",
      "  17. Total Bsmt SF       : |2524.694041|\n",
      "  18. Bsmt Full Bath      : |2408.536233|\n",
      "  19. Sale Condition      : |2318.726511|\n",
      "  20. Screen Porch        : |2005.900171|\n",
      "  21. 1st Flr SF          : |1740.608145|\n",
      "  22. Functional          : |1450.410041|\n",
      "  23. PID                 : |1428.339821|\n",
      "  24. Wood Deck SF        : |1329.009551|\n",
      "  25. Roof Style          : |1220.886054|\n",
      "  26. Pool QC             : |1136.536207|\n",
      "  27. Year Remod/Add      : |971.006213|\n",
      "  28. Fireplace Qu        : |970.583774|\n",
      "  29. Heating QC          : |868.292494|\n",
      "  30. Kitchen AbvGr       : |580.791522|\n",
      "  31. Land Slope          : |521.091114|\n",
      "  32. Bldg Type           : |457.282591|\n",
      "  33. Lot Shape           : |383.500861|\n",
      "  34. Neighborhood        : |366.178908|\n",
      "  35. Garage Finish       : |365.659129|\n",
      "  36. BsmtFin Type 1      : |359.113375|\n",
      "  37. Mas Vnr Type        : |239.502254|\n",
      "  38. Roof Matl           : |72.109021|\n",
      "\n",
      "üîÑ Evaluando Lasso selection...\n",
      "‚úÖ Lasso selection (38 features):\n",
      "   RMSE: $26,090 ¬± $4,264\n",
      "   R¬≤:   0.8908 ¬± 0.0298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# ========== LASSO PARA FEATURE SELECTION ==========\n",
    "print(\"\\n=== EMBEDDED METHOD: Lasso (L1 Regularization) ===\")\n",
    "print(\"Lasso penaliza coeficientes, forzando a 0 features no importantes\")\n",
    "print(\"‚è±Ô∏è Esto puede tomar 30-60 segundos...\\n\")\n",
    "\n",
    "lasso = LassoCV(cv=5, random_state=42, max_iter=1000, n_jobs=-1)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "print(f\"‚úÖ Lasso alpha seleccionado: {lasso.alpha_:.4f}\")\n",
    "\n",
    "# Features seleccionadas (coef != 0)\n",
    "lasso_nonzero = X.columns[lasso.coef_ != 0]\n",
    "print(f\"\\nüìä Features con coeficiente no-cero: {len(lasso_nonzero)} de {X.shape[1]}\")\n",
    "\n",
    "# Seleccionar top-k por magnitud de coeficiente\n",
    "coef_abs = pd.Series(np.abs(lasso.coef_), index=X.columns).sort_values(ascending=False)\n",
    "lasso_features = coef_abs.nlargest(k).index\n",
    "\n",
    "print(f\"\\n‚úÖ Top {k} features por magnitud de coeficiente Lasso:\")\n",
    "for i, (feat, coef) in enumerate(coef_abs.nlargest(k).items(), 1):\n",
    "    print(f\"  {i:2d}. {feat:20s}: |{coef:.6f}|\")\n",
    "\n",
    "# Visualizar coeficientes (top 30)\n",
    "plt.figure(figsize=(14, 10))\n",
    "coef_abs.head(30).sort_values(ascending=True).plot(kind='barh', color='purple')\n",
    "plt.xlabel('|Coeficiente Lasso|', fontsize=12)\n",
    "plt.title('Top 30 Features por Magnitud de Coeficiente Lasso\\n(Mayor magnitud = Mayor importancia)', fontsize=14)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-lasso_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Preparar features para evaluaci√≥n\n",
    "X_lasso = X_scaled[:, X.columns.isin(lasso_features)]\n",
    "\n",
    "# Evaluar con Random Forest\n",
    "print(f\"\\nüîÑ Evaluando Lasso selection...\")\n",
    "rf_lasso = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=15, n_jobs=-1)\n",
    "\n",
    "scores_mse_lasso = -cross_val_score(rf_lasso, X_lasso, y, cv=5, \n",
    "                                     scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "scores_r2_lasso = cross_val_score(rf_lasso, X_lasso, y, cv=5, \n",
    "                                  scoring='r2', n_jobs=-1)\n",
    "rmse_lasso = np.sqrt(scores_mse_lasso)\n",
    "\n",
    "print(f\"‚úÖ Lasso selection ({k} features):\")\n",
    "print(f\"   RMSE: ${rmse_lasso.mean():,.0f} ¬± ${rmse_lasso.std():,.0f}\")\n",
    "print(f\"   R¬≤:   {scores_r2_lasso.mean():.4f} ¬± {scores_r2_lasso.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca7678",
   "metadata": {},
   "source": [
    "## ü§î Preguntas Reflexivas (Lasso)\n",
    "\n",
    "**¬øLasso forz√≥ muchas features a 0? ¬øQu√© te dice esto sobre la redundancia de features?**\n",
    "\n",
    "Si Lasso forz√≥ muchas features a 0, indica alta redundancia en el dataset. Los coeficientes en 0 son features que el modelo L1 considera no aportan informaci√≥n √∫nica. En bienes ra√≠ces, es com√∫n tener features altamente correlacionadas (ej: GarageArea y GarageCars).\n",
    "\n",
    "**¬øLas features que Lasso considera importantes coinciden con los otros m√©todos?**\n",
    "\n",
    "Pueden diferir porque Lasso captura relaciones lineales y penaliza, mientras RF Importance y otros consideran dependencias no-lineales. La intersecci√≥n entre m√©todos es se√±al de features robustas y determinantes para el precio.\n",
    "\n",
    "**¬øLasso es m√°s √∫til para interpretabilidad o para performance?**\n",
    "\n",
    "Lasso ofrece ambas: interpretabilidad (coeficientes) y performance (regularizaci√≥n). Para regresi√≥n de precios, la interpretabilidad permite explicar a clientes qu√© caracter√≠sticas influyen.\n",
    "\n",
    "## ü§î Preguntas Reflexivas (Final de Parte 6)\n",
    "\n",
    "**¬øQu√© m√©todo dio mejor RMSE? ¬øEs el que recomendar√≠as para producci√≥n?**\n",
    "\n",
    "El mejor RMSE lo da t√≠picamente el modelo original o PCA Componentes, pero en producci√≥n priorizo interpretabilidad: Forward/RFE o Feature Importance mantienen features originales y permiten explicar decisiones.\n",
    "\n",
    "**¬øPor qu√© Feature Selection es preferible a PCA en bienes ra√≠ces?**\n",
    "\n",
    "Feature Selection preserva interpretabilidad (\"GrLivArea es importante\"); PCA la pierde con combinaciones lineales. En bienes ra√≠ces, la interpretabilidad es crucial para confianza, transparencia y regulaci√≥n.\n",
    "\n",
    "**Si presentaras estos resultados al CEO (no t√©cnico), ¬øqu√© 3 puntos destacar√≠as?**\n",
    "\n",
    "1. Reducci√≥n dimensional viable: 38 features (‚âà53%) retienen ‚âà80% de la informaci√≥n, con RMSE controlado.\n",
    "2. Interpretabilidad vs precisi√≥n: Feature Selection apenas pierde vs PCA y permite explicar por qu√© un inmueble vale X.\n",
    "3. Trade-off: 2.6% de p√©rdida de precisi√≥n a cambio de 53% menos complejidad y decisiones explicables.\n",
    "\n",
    "**¬øC√≥mo comunicar√≠as el trade-off entre reducir features (velocidad) y mantener precisi√≥n (RMSE)?**\n",
    "\n",
    "\"Usando 38 de las 81 caracter√≠sticas m√°s relevantes, el modelo mantiene el error por debajo de un 2.6% adicional, con la ventaja de decisiones explicables y c√°lculo m√°s r√°pido; la reducci√≥n de latencia compensa la peque√±a p√©rdida de precisi√≥n.\"\n",
    "\n",
    "**¬øEl m√©todo ganador es interpretable? ¬øPor qu√© es cr√≠tico?**\n",
    "\n",
    "S√≠, Feature Selection (Forward/RFE/RF Importance) es interpretable. Es cr√≠tico porque los stakeholders necesitan entender c√≥mo se val√∫a un inmueble para confiar, cumplir normativas y justificar financiamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf462b",
   "metadata": {},
   "source": [
    "## Parte 7: Investigaci√≥n Libre (opcional, +15 min)¬∂\n",
    "Explora uno de estos temas avanzados:\n",
    "\n",
    "## Opci√≥n A: Forward/Backward Selection¬∂\n",
    "Implementa Sequential Feature Selection y compara con RFE:\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "#Forward selection\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    n_features_to_select=k,\n",
    "    direction='forward',\n",
    "    cv=3\n",
    ")\n",
    "#Implementa y eval√∫a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22787808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PARTE 7: FORWARD/BACKWARD SELECTION ==="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Forward Selection...\n",
      "‚úÖ Forward Selection completado en 688.0s\n",
      "Features seleccionadas: 19\n",
      "RMSE: $27,278 ¬± $2,964\n",
      "\n",
      "2. Comparaci√≥n con RFE:\n",
      "Forward: 688.0s\n",
      "RFE: 22.1s\n",
      "\n",
      "üí° Observaci√≥n: Ambos m√©todos son similares en estrategia pero difieren en implementaci√≥n.\n"
     ]
    }
   ],
   "source": [
    "# Parte 7: Forward/Backward Selection\n",
    "print(\"\\n=== PARTE 7: FORWARD/BACKWARD SELECTION ===\")\n",
    "\n",
    "# Forward\n",
    "print(\"\\n1. Forward Selection...\")\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    RandomForestRegressor(random_state=42, n_estimators=20, max_depth=10, n_jobs=-1),\n",
    "    n_features_to_select=k_wrapper,\n",
    "    direction='forward',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "X_sfs_forward = sfs_forward.fit_transform(X_pca_selected, y)\n",
    "forward_time = time.time() - start_time\n",
    "print(f\"‚úÖ Forward Selection completado en {forward_time:.1f}s\")\n",
    "\n",
    "selected_features_sfs_forward = [selected_features_pca[i] for i, sel in enumerate(sfs_forward.get_support()) if sel]\n",
    "print(f\"Features seleccionadas: {len(selected_features_sfs_forward)}\")\n",
    "\n",
    "# Evaluar\n",
    "rf_sfs_forward = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=15, n_jobs=-1)\n",
    "rmse_sfs_forward = np.sqrt(-cross_val_score(rf_sfs_forward, X_sfs_forward, y, cv=5, \n",
    "                                             scoring='neg_mean_squared_error', n_jobs=-1))\n",
    "print(f\"RMSE: ${rmse_sfs_forward.mean():,.0f} ¬± ${rmse_sfs_forward.std():,.0f}\")\n",
    "\n",
    "# Comparar con RFE\n",
    "print(\"\\n2. Comparaci√≥n con RFE:\")\n",
    "print(f\"Forward: {forward_time:.1f}s\")\n",
    "print(f\"RFE: {elapsed_time:.1f}s\")\n",
    "\n",
    "# Visualizaci√≥n comparativa\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "methods = ['Sequential Forward', 'RFE']\n",
    "times = [forward_time, elapsed_time]\n",
    "ax.bar(methods, times, color=['skyblue', 'lightcoral'])\n",
    "ax.set_ylabel('Tiempo (s)', fontsize=12)\n",
    "ax.set_title('Comparaci√≥n de Tiempo: Forward Selection vs RFE', fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizaciones/10-forward_rfe_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nüí° Observaci√≥n: Ambos m√©todos son similares en estrategia pero difieren en implementaci√≥n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c4435",
   "metadata": {},
   "source": [
    "# üìù Reflexi√≥n Final Integradora (OBLIGATORIO)¬∂\n",
    "# ‚è±Ô∏è Tiempo estimado: 15-20 minutos\n",
    "\n",
    "Responde las siguientes preguntas en una celda Markdown al final de tu notebook. Estas preguntas te ayudar√°n a consolidar tu comprensi√≥n y demostrar pensamiento cr√≠tico.\n",
    "\n",
    "## A. Sobre PCA¬∂\n",
    "Interpretabilidad: ¬øPuedes explicar en t√©rminos simples qu√© representa PC1 en el contexto de precios de casas? ¬øEs esto √∫til para un agente inmobiliario?\n",
    "\n",
    "Varianza Explicada: Si PC1 captura 40% de varianza, ¬øqu√© significa esto exactamente? ¬øQu√© informaci√≥n se \"pierde\" en el 60% restante?\n",
    "\n",
    "Cu√°ndo usar PCA: Menciona 3 escenarios reales donde PCA ser√≠a M√ÅS √∫til que Feature Selection (no en este dataset, sino en general).\n",
    "\n",
    "Limitaciones: ¬øCu√°l es la mayor desventaja de PCA para este problema de bienes ra√≠ces?\n",
    "\n",
    "## B. Sobre Feature Selection¬∂\n",
    "Consistencia: Si F-test, MI, RFE y Lasso eligieron features diferentes, ¬øc√≥mo decides cu√°l conjunto usar? ¬øQu√© estrategia aplicar√≠as?\n",
    "\n",
    "Features Redundantes: Si GarageArea y GarageCars est√°n altamente correlacionadas, ¬øcu√°l deber√≠as eliminar? ¬øC√≥mo decides?\n",
    "\n",
    "M√©todos Filter vs Wrapper: ¬øPor qu√© RFE es m√°s lento que F-test? ¬øCu√°ndo justifica el tiempo extra?\n",
    "\n",
    "Lasso Shrinkage: Si Lasso forz√≥ 40 features a coeficiente 0, ¬øqu√© te dice esto sobre la redundancia en el dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21e9c8",
   "metadata": {},
   "source": [
    "# üìù Reflexi√≥n Final Integradora\n",
    "\n",
    "## A. Sobre PCA\n",
    "\n",
    "**Interpretabilidad**: PC1 captura la variabilidad m√°s relevante del dataset; en bienes ra√≠ces, combinaciones lineales de tama√±o, calidad y ubicaci√≥n. Para un agente, esto es poco √∫til porque no puede explicar al cliente por qu√© su casa vale X bas√°ndose en un n√∫mero abstracto.\n",
    "\n",
    "**Varianza Explicada**: PC1 con 40% varianza retiene la cuarta parte de la informaci√≥n multivariada. El 60% restante incluye variaciones de menor impacto pero a√∫n significativas; p√©rdida inevitable, coste de la reducci√≥n dimensional.\n",
    "\n",
    "**Cu√°ndo usar PCA**:\n",
    "1. Procesamiento de im√°genes: reduce dimensiones manteniendo informaci√≥n visual y velocidad de inferencia.\n",
    "2. Microarrays gen√≥micos: miles de genes correlacionados; PCA comprime y preserva patrones biol√≥gicos.\n",
    "3. B√∫squeda de patrones ambientales: m√∫ltiples sensores capturan redundancias; PCA extrae se√±ales naturales.\n",
    "\n",
    "**Limitaciones**: En bienes ra√≠ces, los componentes principales no son interpretables; no puedes decir ‚ÄúPC1 influye m√°s en el precio‚Äù porque los stakeholders necesitan atributos concretos (metros cuadrados, ubicaci√≥n, calidad).\n",
    "\n",
    "## B. Sobre Feature Selection\n",
    "\n",
    "**Consistencia**: Si los m√©todos difieren, priorizo la intersecci√≥n de los m√°s interpretables (F-test, RF Importance, Lasso); uso un ensamble de 2-3 m√©todos por robustez y verifico la estabilidad con CV.\n",
    "\n",
    "**Features Redundantes**: Entre GarageArea y GarageCars correlacionadas, elimino la m√°s d√©bil seg√∫n importance/shapiro (mayor varianza de errores) y contexto; en este caso, mantengo GarageArea por continuidad. Si ambas aportan, conservo las dos hasta validar p√©rdida de precisi√≥n.\n",
    "\n",
    "**Filter vs Wrapper**: RFE es m√°s lento porque entrena el modelo en cada iteraci√≥n, mientras F-test calcula estad√≠sticas en O(n). El tiempo extra se justifica con menos de ~100 variables para capturar interacciones modelo-espec√≠ficas y mejorar la generalizaci√≥n.\n",
    "\n",
    "**Lasso Shrinkage**: Si Lasso elimin√≥ 40 de 81 features, indica redundancia fuerte y estructura lineal subyacente; el dataset es altamente colineal. Esto gu√≠a decisiones de encoding, combinaci√≥n de variables y viabilidad de modelos m√°s simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358422c9",
   "metadata": {},
   "source": [
    "# Trabajos Domiciliarios (Opcionales pero Recomendados)¬∂\n",
    "## ‚è±Ô∏è Tiempo estimado: 2-4 horas por trabajo\n",
    "\n",
    "### Estos trabajos te permitir√°n profundizar en aspectos espec√≠ficos de PCA y Feature Selection. Puedes elegir uno, varios, o todos.\n",
    "\n",
    "### Trabajo Domiciliario 1: Comparaci√≥n de Umbrales de Varianza en PCA¬∂\n",
    "Objetivo: Entender c√≥mo el umbral de varianza afecta performance y complejidad.\n",
    "\n",
    "Tarea: 1. Implementa PCA con diferentes umbrales de varianza explicada: 70%, 80%, 90%, 95%, 99% 2. Para cada umbral: - Registra n√∫mero de componentes - Calcula RMSE y R¬≤ con cross-validation - Mide tiempo de entrenamiento e inferencia 3. Grafica: - Gr√°fico 1: Varianza explicada vs N√∫mero de componentes - Gr√°fico 2: RMSE vs N√∫mero de componentes - Gr√°fico 3: Tiempo de entrenamiento vs N√∫mero de componentes 4. Responde: - ¬øD√≥nde est√° el \"punto √≥ptimo\" (elbow) en el trade-off varianza-performance? - ¬øA partir de cu√°ntos componentes el RMSE deja de mejorar significativamente? - ¬øCu√°nto tiempo ahorras en inferencia con 70% vs 99% varianza? - ¬øQu√© umbral recomendar√≠as para una app m√≥vil que necesita respuestas instant√°neas?\n",
    "\n",
    "Entregable: Notebook con c√≥digo nombre: 03-1-Practica_10.ipynb, gr√°ficos y an√°lisis (max 3 p√°ginas)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
