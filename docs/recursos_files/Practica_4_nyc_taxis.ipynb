{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac68fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completo para análisis multi-fuentes!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Setup completo para análisis multi-fuentes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33039ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos oficiales de NYC Taxi (dataset completo)...\n",
      "   Viajes cargados: 3,066,766 filas, 19 columnas\n",
      "   Columnas: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "   Período: 2008-12-31 23:01:42 a 2023-02-01 00:56:53\n",
      "   Tamaño en memoria: 565.6 MB\n",
      "\n",
      "Cargando datos oficiales de zonas NYC...\n",
      "   Zonas cargadas: 265 filas, 4 columnas\n",
      "   Columnas: ['LocationID', 'Borough', 'Zone', 'service_zone']\n",
      "   Boroughs únicos: ['EWR' 'Queens' 'Bronx' 'Manhattan' 'Staten Island' 'Brooklyn' 'Unknown'\n",
      " nan]\n",
      "\n",
      "Cargando datos de calendario de eventos...\n",
      "   Eventos calendario: 3 filas\n",
      "   Columnas: ['date', 'name', 'special']\n",
      "\n",
      "VISTA PREVIA DE DATOS:\n",
      "\n",
      "--- TRIPS ---\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2          9.3   1.00      0.5        0.00           0.0   \n",
      "1             1          7.9   1.00      0.5        4.00           0.0   \n",
      "2             1         14.9   1.00      0.5       15.00           0.0   \n",
      "3             1         12.1   7.25      0.5        0.00           0.0   \n",
      "4             1         11.4   1.00      0.5        3.28           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         14.30                   2.5         0.00  \n",
      "1                    1.0         16.90                   2.5         0.00  \n",
      "2                    1.0         34.90                   2.5         0.00  \n",
      "3                    1.0         20.85                   0.0         1.25  \n",
      "4                    1.0         19.68                   2.5         0.00  \n",
      "\n",
      "--- ZONES ---\n",
      "   LocationID        Borough                     Zone service_zone\n",
      "0           1            EWR           Newark Airport          EWR\n",
      "1           2         Queens              Jamaica Bay    Boro Zone\n",
      "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
      "3           4      Manhattan            Alphabet City  Yellow Zone\n",
      "4           5  Staten Island            Arden Heights    Boro Zone\n",
      "\n",
      "--- CALENDAR ---\n",
      "         date       name  special\n",
      "0  2022-01-01   New Year     True\n",
      "1  2022-01-03  Event Day     True\n",
      "2  2022-01-05  Promo Day     True\n"
     ]
    }
   ],
   "source": [
    "# === CARGAR DATOS DE MÚLTIPLES FUENTES ===\n",
    "\n",
    "# 1. Cargar datos de viajes desde Parquet (Dataset oficial completo NYC)\n",
    "print(\"Cargando datos oficiales de NYC Taxi (dataset completo)...\")\n",
    "trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Cargar dataset oficial (~3M registros de enero 2023) # función para leer archivos .parquet (más eficiente que CSV)\n",
    "trips = pd.read_parquet(trips_url, engine=\"fastparquet\")\n",
    "\n",
    "print(f\"   Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(trips.columns)}\")\n",
    "print(f\"   Período: {trips['tpep_pickup_datetime'].min()} a {trips['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"   Tamaño en memoria: {trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# 2. Cargar datos de zonas desde CSV (Dataset oficial completo)\n",
    "print(\"\\nCargando datos oficiales de zonas NYC...\")\n",
    "zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "zones = pd.read_csv(zones_url)  # función estándar para archivos CSV\n",
    "\n",
    "print(f\"   Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(zones.columns)}\")\n",
    "print(f\"   Boroughs únicos: {zones['Borough'].unique()}\")\n",
    "\n",
    "# 3. Cargar calendario de eventos desde JSON \n",
    "print(\"\\nCargando datos de calendario de eventos...\")\n",
    "calendar_url = \"https://juanfkurucz.com/ucu-id/ut1/data/calendar.json\"\n",
    "calendar = pd.read_json(calendar_url)  # función para archivos JSON\n",
    "calendar['date'] = pd.to_datetime(calendar['date']).dt.date  # convertir strings a fechas, luego extraer solo la fecha\n",
    "\n",
    "print(f\"   Eventos calendario: {calendar.shape[0]} filas\")\n",
    "print(f\"   Columnas: {list(calendar.columns)}\")\n",
    "\n",
    "# 4. Mostrar primeras filas de cada dataset\n",
    "print(\"\\nVISTA PREVIA DE DATOS:\")\n",
    "print(\"\\n--- TRIPS ---\")\n",
    "print(trips.head())  # método para mostrar primeras filas de un DataFrame\n",
    "print(\"\\n--- ZONES ---\")\n",
    "print(zones.head())  # mismo método para ver estructura de datos\n",
    "print(\"\\n--- CALENDAR ---\")\n",
    "print(calendar.head())  # revisar formato de los eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4263d845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando nombres de columnas...\n",
      "   Trips columnas: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "   Zones columnas: ['locationid', 'borough', 'zone', 'service_zone']\n",
      "   Columna pickup_date creada\n",
      "   Rango de fechas: 2008-12-31 a 2023-02-01\n",
      "\n",
      "VERIFICACIÓN DE TIPOS PARA JOINS:\n",
      "   trips['pulocationid'] tipo: int64\n",
      "   zones['locationid'] tipo: int64\n",
      "   trips['pickup_date'] tipo: <class 'datetime.date'>\n",
      "   calendar['date'] tipo: <class 'datetime.date'>\n",
      "\n",
      "OPTIMIZACIÓN PARA DATASETS GRANDES:\n",
      "   Memoria inicial: 682.6 MB\n",
      "   Optimizando tipos de datos para 3M+ registros...\n",
      "   Limpiando valores nulos antes de optimización...\n",
      "   Registros después de limpieza: 3,066,766\n",
      "   Memoria optimizada: 627.0 MB\n",
      "   Ahorro de memoria: 8.1%\n",
      "\n",
      "DATOS FALTANTES ANTES DE JOINS:\n",
      "Trips (top 5 columnas con más nulos):\n",
      "airport_fee             71743\n",
      "congestion_surcharge    71743\n",
      "store_and_fwd_flag      71743\n",
      "ratecodeid              71743\n",
      "passenger_count             0\n",
      "dtype: int64\n",
      "\n",
      "Zones:\n",
      "locationid      0\n",
      "borough         1\n",
      "zone            1\n",
      "service_zone    2\n",
      "dtype: int64\n",
      "\n",
      "Calendar:\n",
      "date       0\n",
      "name       0\n",
      "special    0\n",
      "dtype: int64\n",
      "\n",
      "ANÁLISIS DE CALIDAD:\n",
      "   Total de viajes: 3,066,766\n",
      "   Viajes sin pickup location: 0\n",
      "   Viajes sin dropoff location: 0\n",
      "   Viajes sin passenger_count: 0\n",
      "\n",
      "ESTRATEGIAS DE LIMPIEZA:\n",
      "   Ubicaciones nulas: Eliminar (crítico para joins)\n",
      "   Passenger_count nulos: Rellenar con valor típico (1)\n",
      "   Tarifas nulas: Revisar caso por caso\n"
     ]
    }
   ],
   "source": [
    "# === NORMALIZAR Y PREPARAR DATOS PARA JOINS ===\n",
    "\n",
    "# 1. Estandarizar nombres de columnas\n",
    "print(\"Normalizando nombres de columnas...\")\n",
    "trips.columns = trips.columns.str.lower()  # convertir todas las columnas a minúsculas\n",
    "zones.columns = zones.columns.str.lower()  # misma transformación para consistencia\n",
    "\n",
    "print(f\"   Trips columnas: {list(trips.columns)}\")\n",
    "print(f\"   Zones columnas: {list(zones.columns)}\")\n",
    "\n",
    "# 2. Crear columna de fecha para el join con calendario\n",
    "trips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date  # extraer solo la fecha (sin hora) de la columna datetime\n",
    "\n",
    "print(f\"   Columna pickup_date creada\")\n",
    "print(f\"   Rango de fechas: {trips['pickup_date'].min()} a {trips['pickup_date'].max()}\")\n",
    "\n",
    "# 3. Verificar tipos de datos para joins\n",
    "print(\"\\nVERIFICACIÓN DE TIPOS PARA JOINS:\")\n",
    "print(f\"   trips['pulocationid'] tipo: {trips['pulocationid'].dtype}\")\n",
    "print(f\"   zones['locationid'] tipo: {zones['locationid'].dtype}\")\n",
    "print(f\"   trips['pickup_date'] tipo: {type(trips['pickup_date'].iloc[0])}\")\n",
    "print(f\"   calendar['date'] tipo: {type(calendar['date'].iloc[0])}\")\n",
    "\n",
    "# 4. Optimización para datasets grandes (~3M registros)\n",
    "print(\"\\nOPTIMIZACIÓN PARA DATASETS GRANDES:\")\n",
    "initial_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"   Memoria inicial: {initial_memory:.1f} MB\")\n",
    "\n",
    "# Optimizar tipos de datos para 3+ millones de registros\n",
    "print(\"   Optimizando tipos de datos para 3M+ registros...\")\n",
    "\n",
    "# Limpiar valores nulos antes de convertir tipos\n",
    "print(\"   Limpiando valores nulos antes de optimización...\")\n",
    "trips['passenger_count'] = trips['passenger_count'].fillna(1)  # método para rellenar valores nulos con un valor específico\n",
    "trips = trips.dropna(subset=['pulocationid', 'dolocationid'])  # eliminar filas críticas sin ubicación (necesarias para joins)\n",
    "\n",
    "# Convertir tipos después de limpiar\n",
    "trips['pulocationid'] = trips['pulocationid'].astype('int16')\n",
    "trips['dolocationid'] = trips['dolocationid'].astype('int16') \n",
    "trips['passenger_count'] = trips['passenger_count'].astype('int8')\n",
    "zones['locationid'] = zones['locationid'].astype('int16')\n",
    "\n",
    "print(f\"   Registros después de limpieza: {len(trips):,}\")\n",
    "\n",
    "optimized_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "savings = ((initial_memory - optimized_memory) / initial_memory * 100)\n",
    "\n",
    "print(f\"   Memoria optimizada: {optimized_memory:.1f} MB\")\n",
    "print(f\"   Ahorro de memoria: {savings:.1f}%\")\n",
    "\n",
    "# 5. Revisar datos faltantes antes de joins\n",
    "print(\"\\nDATOS FALTANTES ANTES DE JOINS:\")\n",
    "print(\"Trips (top 5 columnas con más nulos):\")\n",
    "trips_nulls = trips.isna().sum().sort_values(ascending=False).head()  # método para detectar valores nulos, sumar y ordenar\n",
    "print(trips_nulls)\n",
    "\n",
    "print(\"\\nZones:\")\n",
    "zones_nulls = zones.isna().sum()  # revisar si hay valores faltantes en lookup table\n",
    "print(zones_nulls)\n",
    "\n",
    "print(\"\\nCalendar:\")\n",
    "calendar_nulls = calendar.isna().sum()  # verificar integridad del calendario de eventos\n",
    "print(calendar_nulls)\n",
    "\n",
    "# Análisis de calidad de datos\n",
    "print(\"\\nANÁLISIS DE CALIDAD:\")\n",
    "total_trips = len(trips)\n",
    "print(f\"   Total de viajes: {total_trips:,}\")\n",
    "print(f\"   Viajes sin pickup location: {trips['pulocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin dropoff location: {trips['dolocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin passenger_count: {trips['passenger_count'].isna().sum():,}\")\n",
    "\n",
    "# Estrategias de limpieza recomendadas\n",
    "print(\"\\nESTRATEGIAS DE LIMPIEZA:\")\n",
    "print(\"   Ubicaciones nulas: Eliminar (crítico para joins)\")\n",
    "print(\"   Passenger_count nulos: Rellenar con valor típico (1)\")\n",
    "print(\"   Tarifas nulas: Revisar caso por caso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eed1071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros en trips: 2995023\n"
     ]
    }
   ],
   "source": [
    "#Eliminar los datos nulos (71000 de 3000000 es razonable)\n",
    "trips = trips.dropna()\n",
    "print(f\"Registros en trips: {len(trips)}\")\n",
    "\n",
    "# Rellenar valores nulos en las columnas de zones con 'Desconocido'\n",
    "zones['borough'] = zones['borough'].fillna('Desconocido')\n",
    "zones['zone'] = zones['zone'].fillna('Desconocido')\n",
    "zones['service_zone'] = zones['service_zone'].fillna('Desconocido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79fcf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando join: trips + zones...\n",
      "   Registros antes del join: 2995023\n",
      "   Registros después del join: 2995023\n",
      "   Nuevas columnas añadidas: ['locationid', 'borough', 'zone', 'service_zone']\n",
      "\n",
      "VERIFICACIÓN DEL JOIN:\n",
      "Conteo por Borough:\n",
      "borough\n",
      "Manhattan        2648320\n",
      "Queens            285126\n",
      "Unknown            39788\n",
      "Brooklyn           15478\n",
      "Bronx               3931\n",
      "Desconocido         1632\n",
      "EWR                  409\n",
      "Staten Island        339\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Viajes sin borough asignado: 0\n",
      "\n",
      "MUESTRA DEL DATASET INTEGRADO:\n",
      "   pulocationid    borough               zone  trip_distance  total_amount\n",
      "0           161  Manhattan     Midtown Center           0.97         14.30\n",
      "1            43  Manhattan       Central Park           1.10         16.90\n",
      "2            48  Manhattan       Clinton East           2.51         34.90\n",
      "3           138     Queens  LaGuardia Airport           1.90         20.85\n",
      "4           107  Manhattan           Gramercy           1.43         19.68\n"
     ]
    }
   ],
   "source": [
    "# === PRIMER JOIN: TRIPS + ZONES ===\n",
    "\n",
    "# 1. Hacer join de trips con zones para obtener información geográfica\n",
    "print(\"Realizando join: trips + zones...\")\n",
    "trips_with_zones = trips.merge(zones,   # método principal para unir DataFrames\n",
    "                                left_on='pulocationid',   # columna de trips que contiene ID de zona de pickup\n",
    "                                right_on='locationid',  # columna de zones que contiene ID correspondiente\n",
    "                                how='left')       # tipo de join que mantiene todos los trips\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips)}\")\n",
    "print(f\"   Registros después del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Nuevas columnas añadidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}\")\n",
    "\n",
    "# 2. Verificar el resultado del join\n",
    "print(\"\\nVERIFICACIÓN DEL JOIN:\")\n",
    "print(\"Conteo por Borough:\")\n",
    "print(trips_with_zones['borough'].value_counts())\n",
    "\n",
    "# 3. Verificar si hay valores nulos después del join\n",
    "null_after_join = trips_with_zones['borough'].isna().sum()  # contar nulos en columna borough\n",
    "print(f\"\\nViajes sin borough asignado: {null_after_join}\")\n",
    "\n",
    "if null_after_join > 0:\n",
    "    print(\"   Algunos viajes no encontraron su zona correspondiente\")\n",
    "    print(\"   LocationIDs problemáticos:\")\n",
    "    problematic_ids = trips_with_zones[trips_with_zones['borough'].isna()]['pulocationid'].unique()  # filtrar filas con nulos\n",
    "    print(f\"   {problematic_ids}\")\n",
    "\n",
    "# 4. Mostrar muestra del resultado\n",
    "print(\"\\nMUESTRA DEL DATASET INTEGRADO:\")\n",
    "print(trips_with_zones[['pulocationid', 'borough', 'zone', 'trip_distance', 'total_amount']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1e5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando join: trips_zones + calendar...\n",
      "   Registros antes del join: 2995023\n",
      "   Registros después del join: 2995023\n",
      "\n",
      "DISTRIBUCIÓN DE DÍAS ESPECIALES:\n",
      "is_special_day\n",
      "False    2995023\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ejemplos de eventos especiales:\n",
      "   No hay eventos especiales en este período\n",
      "\n",
      "DATASET FINAL INTEGRADO:\n",
      "   Total registros: 2995023\n",
      "   Total columnas: 28\n",
      "   Columnas principales: ['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']\n",
      "\n",
      "VERIFICACIÓN FINAL:\n",
      "Datos faltantes por columna clave:\n",
      "   borough: 0 nulos\n",
      "   zone: 0 nulos\n",
      "   trip_distance: 0 nulos\n",
      "   total_amount: 0 nulos\n",
      "   is_special_day: 0 nulos\n"
     ]
    }
   ],
   "source": [
    "# === SEGUNDO JOIN: TRIPS_ZONES + CALENDAR ===\n",
    "\n",
    "# 1. Hacer join con datos de calendario\n",
    "print(\"Realizando join: trips_zones + calendar...\")\n",
    "trips_complete = trips_with_zones.merge(calendar,   # mismo método de join que antes\n",
    "                                         left_on='pickup_date',   # columna de fecha que creamos en trips\n",
    "                                         right_on='date',  # columna de fecha en calendar\n",
    "                                         how='left')       # tipo que mantiene todos los trips aunque no haya evento especial\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Registros después del join: {len(trips_complete)}\")\n",
    "\n",
    "# 2. Crear flag de evento especial\n",
    "trips_complete['is_special_day'] = trips_complete['special'].fillna('False')  # método para rellenar nulos con valor por defecto\n",
    "\n",
    "print(\"\\nDISTRIBUCIÓN DE DÍAS ESPECIALES:\")\n",
    "print(trips_complete['is_special_day'].value_counts())\n",
    "print(\"\\nEjemplos de eventos especiales:\")\n",
    "special_days = trips_complete[trips_complete['is_special_day'] == True]\n",
    "if len(special_days) > 0:\n",
    "    print(special_days[['pickup_date', 'special', 'borough']].drop_duplicates())\n",
    "else:\n",
    "    print(\"   No hay eventos especiales en este período\")\n",
    "\n",
    "# 3. Mostrar dataset final integrado\n",
    "print(\"\\nDATASET FINAL INTEGRADO:\")\n",
    "print(f\"   Total registros: {len(trips_complete)}\")\n",
    "print(f\"   Total columnas: {len(trips_complete.columns)}\")\n",
    "print(f\"   Columnas principales: {['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']}\")\n",
    "\n",
    "# 4. Verificar integridad de los datos finales\n",
    "print(\"\\nVERIFICACIÓN FINAL:\")\n",
    "print(\"Datos faltantes por columna clave:\")\n",
    "key_columns = ['borough', 'zone', 'trip_distance', 'total_amount', 'is_special_day']\n",
    "for col in key_columns:\n",
    "    missing = trips_complete[col].isna().sum()  # verificar nulos en cada columna clave final\n",
    "    print(f\"   {col}: {missing} nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a798f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis por Borough (procesando datos grandes)...\n",
      "\n",
      "ANÁLISIS COMPLETO POR BOROUGH:\n",
      "               num_trips  avg_distance  std_distance  median_distance  \\\n",
      "borough                                                                 \n",
      "Manhattan        2648320          2.41         40.73             1.61   \n",
      "Queens            285126         12.32         12.31            11.27   \n",
      "Unknown            39788          7.59        145.56             2.63   \n",
      "Brooklyn           15478          4.48          4.94             3.03   \n",
      "Bronx               3931          5.19          6.40             2.90   \n",
      "Desconocido         1632          2.34          7.56             0.00   \n",
      "EWR                  409          1.60          5.68             0.00   \n",
      "Staten Island        339         11.34         10.24            14.80   \n",
      "\n",
      "               avg_total  std_total  median_total  avg_fare  avg_tip  \\\n",
      "borough                                                                \n",
      "Manhattan          22.35      14.47         19.00     14.64     2.86   \n",
      "Queens             67.35      33.65         70.45     50.04     7.85   \n",
      "Unknown            38.12      30.48         25.30     26.47     4.82   \n",
      "Brooklyn           32.02      23.17         27.00     26.20     2.62   \n",
      "Bronx              34.15      33.83         29.00     30.01     0.65   \n",
      "Desconocido       108.20      96.90         91.20     95.08     9.93   \n",
      "EWR               104.34      62.82        118.50     87.99    12.43   \n",
      "Staten Island      62.44      45.04         67.80     48.68     1.29   \n",
      "\n",
      "               median_tip  avg_passengers  \n",
      "borough                                    \n",
      "Manhattan            2.66            1.36  \n",
      "Queens               8.18            1.39  \n",
      "Unknown              3.14            1.35  \n",
      "Brooklyn             0.00            1.26  \n",
      "Bronx                0.00            1.10  \n",
      "Desconocido          0.01            1.43  \n",
      "EWR                 10.00            1.58  \n",
      "Staten Island        0.00            1.13  \n",
      "\n",
      "ANÁLISIS CON MÉTRICAS EMPRESARIALES:\n",
      "               num_trips  market_share  revenue_per_km  tip_rate\n",
      "borough                                                         \n",
      "Manhattan        2648320          88.4            9.27      19.5\n",
      "Queens            285126           9.5            5.47      15.7\n",
      "Unknown            39788           1.3            5.02      18.2\n",
      "Brooklyn           15478           0.5            7.15      10.0\n",
      "Bronx               3931           0.1            6.58       2.2\n",
      "Desconocido         1632           0.1           46.24      10.4\n",
      "EWR                  409           0.0           65.21      14.1\n",
      "Staten Island        339           0.0            5.51       2.6\n",
      "\n",
      "INSIGHTS PRINCIPALES:\n",
      "   Borough con más viajes: Manhattan\n",
      "   Borough con viajes más largos: Queens\n",
      "   Borough con tarifas más altas: ['Desconocido', 'EWR', 'Queens']\n",
      "   Mejor revenue por km: EWR\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS AGREGADO POR BOROUGH ===\n",
    "\n",
    "# 1. Análisis básico por borough (con dataset grande)\n",
    "print(\"Análisis por Borough (procesando datos grandes)...\")\n",
    "borough_analysis = trips_complete.groupby(by='borough').agg({   # método para agrupar datos, por qué columna geográfica?\n",
    "    'pulocationid': 'count',  # función para contar número de registros/viajes\n",
    "    'trip_distance': ['mean', 'std', 'median'],  # función para promedio + desviación + mediana\n",
    "    'total_amount': ['mean', 'std', 'median'],   # mismas estadísticas para tarifas\n",
    "    'fare_amount': 'mean',     # solo promedio de tarifa base\n",
    "    'tip_amount': ['mean', 'median'],  # estadísticas de propinas\n",
    "    'passenger_count': 'mean'  # función para promedio de pasajeros\n",
    "}).round(2)\n",
    "\n",
    "# Aplanar columnas multi-nivel\n",
    "borough_analysis.columns = ['num_trips', 'avg_distance', 'std_distance', 'median_distance',\n",
    "                           'avg_total', 'std_total', 'median_total', 'avg_fare', \n",
    "                           'avg_tip', 'median_tip', 'avg_passengers']\n",
    "\n",
    "# Ordenar por número de viajes\n",
    "borough_analysis = borough_analysis.sort_values(by='num_trips', ascending=False)  # método para ordenar DataFrame por una columna específica\n",
    "\n",
    "print(\"\\nANÁLISIS COMPLETO POR BOROUGH:\")\n",
    "print(borough_analysis)\n",
    "\n",
    "# 2. Calcular métricas adicionales empresariales\n",
    "borough_analysis['revenue_per_km'] = (borough_analysis['avg_total'] / \n",
    "                                     borough_analysis['avg_distance']).round(2)\n",
    "borough_analysis['tip_rate'] = (borough_analysis['avg_tip'] / \n",
    "                               borough_analysis['avg_fare'] * 100).round(1)\n",
    "borough_analysis['market_share'] = (borough_analysis['num_trips'] / \n",
    "                                  borough_analysis['num_trips'].sum() * 100).round(1)\n",
    "\n",
    "print(\"\\nANÁLISIS CON MÉTRICAS EMPRESARIALES:\")\n",
    "print(borough_analysis[['num_trips', 'market_share', 'revenue_per_km', 'tip_rate']])\n",
    "\n",
    "# 3. Encontrar insights\n",
    "print(\"\\nINSIGHTS PRINCIPALES:\")\n",
    "print(f\"   Borough con más viajes: {borough_analysis.index[0]}\")\n",
    "print(f\"   Borough con viajes más largos: {borough_analysis['avg_distance'].idxmax()}\")\n",
    "print(\"   Borough con tarifas más altas:\", borough_analysis.sort_values(by='avg_total', ascending=False).head(3).index.tolist())\n",
    "print(f\"   Mejor revenue por km: {borough_analysis['revenue_per_km'].idxmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a41b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36                1   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27                1   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49                1   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25                0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19                1   \n",
      "\n",
      "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  ...  airport_fee  pickup_date  locationid    borough  \\\n",
      "0             2  ...         0.00   2023-01-01         161  Manhattan   \n",
      "1             1  ...         0.00   2023-01-01          43  Manhattan   \n",
      "2             1  ...         0.00   2023-01-01          48  Manhattan   \n",
      "3             1  ...         1.25   2023-01-01         138     Queens   \n",
      "4             1  ...         0.00   2023-01-01         107  Manhattan   \n",
      "\n",
      "                zone  service_zone  date  name  special is_special_day  \n",
      "0     Midtown Center   Yellow Zone   NaN   NaN      NaN          False  \n",
      "1       Central Park   Yellow Zone   NaN   NaN      NaN          False  \n",
      "2       Clinton East   Yellow Zone   NaN   NaN      NaN          False  \n",
      "3  LaGuardia Airport      Airports   NaN   NaN      NaN          False  \n",
      "4           Gramercy   Yellow Zone   NaN   NaN      NaN          False  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "Análisis: Borough + Día Especial...\n",
      "\n",
      "ANÁLISIS BOROUGH + DÍA ESPECIAL:\n",
      "                              num_trips  avg_distance  avg_total\n",
      "borough       is_special_day                                    \n",
      "Bronx         False                3931          5.19      34.15\n",
      "Brooklyn      False               15478          4.48      32.02\n",
      "Desconocido   False                1632          2.34     108.20\n",
      "EWR           False                 409          1.60     104.34\n",
      "Manhattan     False             2648320          2.41      22.35\n",
      "Queens        False              285126         12.32      67.35\n",
      "Staten Island False                 339         11.34      62.44\n",
      "Unknown       False               39788          7.59      38.12\n",
      "\n",
      "COMPARACIÓN DÍAS NORMALES VS ESPECIALES:\n",
      "            Avg Distance  Avg Amount  Num Trips\n",
      "Día Normal          3.44       26.97    2995023\n",
      "\n",
      "SOLO HAY Día Normal:\n",
      "   Viajes: 2,995,023.0\n",
      "   Distancia promedio: 3.44 millas\n",
      "   Tarifa promedio: $26.97\n",
      "   No hay datos de días especiales para comparar en este período\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS COMPARATIVO: DÍAS NORMALES VS ESPECIALES ===\n",
    "print(trips_complete.head())\n",
    "# 1. Análisis por borough y tipo de día\n",
    "print(\"Análisis: Borough + Día Especial...\")\n",
    "borough_day_analysis = trips_complete.groupby(by=['borough', 'is_special_day']).agg({  # agrupar por DOS columnas: geografía y tipo de día\n",
    "    'pulocationid': 'count',  # función para contar viajes\n",
    "    'trip_distance': 'mean',  # función para promedio de distancia\n",
    "    'total_amount': 'mean'    # función para promedio de tarifa\n",
    "}).round(2)\n",
    "\n",
    "borough_day_analysis.columns = ['num_trips', 'avg_distance', 'avg_total']\n",
    "\n",
    "print(\"\\nANÁLISIS BOROUGH + DÍA ESPECIAL:\")\n",
    "print(borough_day_analysis)\n",
    "\n",
    "# 2. Comparar días normales vs especiales\n",
    "print(\"\\nCOMPARACIÓN DÍAS NORMALES VS ESPECIALES:\")\n",
    "\n",
    "# Pivotear para comparar fácilmente\n",
    "comparison = trips_complete.groupby(by='is_special_day').agg({  # agrupar solo por tipo de día para comparación general\n",
    "    'trip_distance': 'mean',    # promedio de distancia por tipo de día\n",
    "    'total_amount': 'mean',     # promedio de tarifa por tipo de día\n",
    "    'pulocationid': 'count'     # conteo de viajes por tipo de día\n",
    "}).round(2)\n",
    "\n",
    "# Renombrar índices según los valores únicos encontrados\n",
    "unique_day_types = comparison.index.tolist()\n",
    "if len(unique_day_types) == 2:\n",
    "    comparison.index = ['Día Normal', 'Día Especial']\n",
    "elif len(unique_day_types) == 1:\n",
    "    if unique_day_types[0] in ['False', False]:\n",
    "        comparison.index = ['Día Normal']\n",
    "    else:\n",
    "        comparison.index = ['Día Especial']\n",
    "\n",
    "comparison.columns = ['Avg Distance', 'Avg Amount', 'Num Trips']\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "# 3. Calcular diferencias porcentuales\n",
    "if len(comparison) > 1:\n",
    "    # Hay tanto días normales como especiales\n",
    "    if 'Día Normal' in comparison.index and 'Día Especial' in comparison.index:\n",
    "        normal_day = comparison.loc['Día Normal']\n",
    "        special_day = comparison.loc['Día Especial']\n",
    "\n",
    "        print(\"\\nIMPACTO DE DÍAS ESPECIALES:\")\n",
    "        distance_change = ((special_day['Avg Distance'] - normal_day['Avg Distance']) / normal_day['Avg Distance'] * 100)\n",
    "        amount_change = ((special_day['Avg Amount'] - normal_day['Avg Amount']) / normal_day['Avg Amount'] * 100)\n",
    "\n",
    "        print(f\"   Cambio en distancia promedio: {distance_change:+.1f}%\")\n",
    "        print(f\"   Cambio en tarifa promedio: {amount_change:+.1f}%\")\n",
    "    else:\n",
    "        print(\"\\nINFORMACIÓN DE DÍAS:\")\n",
    "        for idx, row in comparison.iterrows():\n",
    "            print(f\"   {idx}: {row['Num Trips']:,} viajes, ${row['Avg Amount']:.2f} promedio\")\n",
    "else:\n",
    "    print(f\"\\nSOLO HAY {comparison.index[0]}:\")\n",
    "    print(f\"   Viajes: {comparison.iloc[0]['Num Trips']:,}\")\n",
    "    print(f\"   Distancia promedio: {comparison.iloc[0]['Avg Distance']:.2f} millas\")\n",
    "    print(f\"   Tarifa promedio: ${comparison.iloc[0]['Avg Amount']:.2f}\")\n",
    "    print(\"   No hay datos de días especiales para comparar en este período\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbde4fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Aplicando técnicas para datasets grandes...\n",
      "Dataset grande detectado: 2,995,023 registros\n",
      "Creando muestra estratificada para visualizaciones...\n",
      "Muestra creada: 10,000 registros (0.3%)\n",
      "\n",
      "ANÁLISIS DE PERFORMANCE:\n",
      "   total_trips: 2,995,023\n",
      "   matched_zones: 2,995,023\n",
      "   match_rate: 100.0%\n",
      "   unique_zones_used: 255\n",
      "   total_zones_available: 265\n",
      "   zone_coverage: 96.2%\n",
      "\n",
      "ANÁLISIS TEMPORAL AVANZADO:\n",
      "Horas pico por número de viajes:\n",
      "      18:00 - 210,761.0 viajes\n",
      "      17:00 - 204,808.0 viajes\n",
      "      15:00 - 193,114.0 viajes\n"
     ]
    }
   ],
   "source": [
    "# === TÉCNICAS PARA TRABAJAR CON DATASETS GRANDES ===\n",
    "\n",
    "# 1. Sampling estratégico para visualizaciones\n",
    "print(\"⚡ Aplicando técnicas para datasets grandes...\")\n",
    "\n",
    "# Si el dataset es muy grande, usar muestra para visualizaciones\n",
    "if len(trips_complete) > 50000:\n",
    "    print(f\"Dataset grande detectado: {len(trips_complete):,} registros\")\n",
    "    print(\"Creando muestra estratificada para visualizaciones...\")\n",
    "\n",
    "    # Muestra proporcional por borough\n",
    "    sample_size = min(10000, len(trips_complete) // 10)\n",
    "    trips_sample = trips_complete.sample(n=sample_size, random_state=42)  # método para tomar muestra aleatoria de n registros\n",
    "\n",
    "    print(f\"Muestra creada: {len(trips_sample):,} registros ({len(trips_sample)/len(trips_complete)*100:.1f}%)\")\n",
    "else:\n",
    "    trips_sample = trips_complete\n",
    "    print(\"Dataset pequeño, usando datos completos para visualización\")\n",
    "\n",
    "# 2. Análisis de performance de joins\n",
    "print(\"\\nANÁLISIS DE PERFORMANCE:\")\n",
    "join_stats = {\n",
    "    'total_trips': len(trips),\n",
    "    'matched_zones': (trips_complete['borough'].notna()).sum(),\n",
    "    'match_rate': (trips_complete['borough'].notna().sum() / len(trips) * 100),\n",
    "    'unique_zones_used': trips_complete['zone'].nunique(),\n",
    "    'total_zones_available': len(zones),\n",
    "    'zone_coverage': (trips_complete['zone'].nunique() / len(zones) * 100)\n",
    "}\n",
    "\n",
    "for key, value in join_stats.items():\n",
    "    if 'rate' in key or 'coverage' in key:\n",
    "        print(f\"   {key}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value:,}\")\n",
    "\n",
    "# 3. Análisis temporal avanzado (solo si hay suficientes datos)\n",
    "if len(trips_complete) > 1000:\n",
    "    print(\"\\nANÁLISIS TEMPORAL AVANZADO:\")\n",
    "\n",
    "    # Análisis por hora del día\n",
    "    trips_complete['pickup_hour'] = trips_complete['tpep_pickup_datetime'].dt.hour  # extraer hora de la fecha/hora\n",
    "    hourly_analysis = trips_complete.groupby(by='pickup_hour').agg({  # agrupar por hora del día\n",
    "        'pulocationid': 'count',     # contar viajes por hora\n",
    "        'total_amount': 'mean',      # tarifa promedio por hora\n",
    "        'trip_distance': 'mean'      # distancia promedio por hora\n",
    "    }).round(2)\n",
    "\n",
    "    hourly_analysis.columns = ['trips_count', 'avg_amount', 'avg_distance']\n",
    "\n",
    "    print(\"Horas pico por número de viajes:\")\n",
    "    peak_hours = hourly_analysis.sort_values(by='trips_count', ascending=False).head(3)  # ordenar por más viajes, tomar top 3\n",
    "    for hour, stats in peak_hours.iterrows():\n",
    "        print(f\"      {hour:02d}:00 - {stats['trips_count']:,} viajes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f2cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando correlaciones entre variables numéricas...\n",
      "\n",
      "Matriz de Correlación:\n",
      "               trip_distance  total_amount  fare_amount  tip_amount\n",
      "trip_distance          1.000         0.094        0.094       0.061\n",
      "total_amount           0.094         1.000        0.980       0.708\n",
      "fare_amount            0.094         0.980        1.000       0.588\n",
      "tip_amount             0.061         0.708        0.588       1.000\n",
      "\n",
      "Correlaciones más fuertes:\n",
      "   total_amount vs fare_amount: 0.980\n",
      "   total_amount vs tip_amount: 0.708\n",
      "   fare_amount vs tip_amount: 0.588\n",
      "\n",
      "INTERPRETACIÓN DE CORRELACIONES:\n",
      "   > 0.7: Correlación fuerte positiva\n",
      "   0.3-0.7: Correlación moderada positiva\n",
      "   -0.3-0.3: Correlación débil\n",
      "   < -0.7: Correlación fuerte negativa\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS DE CORRELACIONES NUMÉRICAS ===\n",
    "\n",
    "# Calcular correlaciones entre variables numéricas\n",
    "print(\"Calculando correlaciones entre variables numéricas...\")\n",
    "numeric_cols = ['trip_distance', 'total_amount', 'fare_amount', 'tip_amount']\n",
    "corr_matrix = trips_complete[numeric_cols].corr()  # método para calcular matriz de correlación\n",
    "\n",
    "print(\"\\nMatriz de Correlación:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "print(\"\\nCorrelaciones más fuertes:\")\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "for var1, var2, corr in corr_pairs[:3]:\n",
    "    print(f\"   {var1} vs {var2}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nINTERPRETACIÓN DE CORRELACIONES:\")\n",
    "print(\"   > 0.7: Correlación fuerte positiva\")\n",
    "print(\"   0.3-0.7: Correlación moderada positiva\") \n",
    "print(\"   -0.3-0.3: Correlación débil\")\n",
    "print(\"   < -0.7: Correlación fuerte negativa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4928312",
   "metadata": {},
   "source": [
    "## PREGUNTAS FINALES\n",
    "1. ¿Qué diferencia hay entre un LEFT JOIN y un INNER JOIN? PISTA: Guía visual de joins\n",
    "\n",
    "Usando el left join te quedas con los datos de la tabla, es la que se mantiene, mientras que de la derecha son los valores que se agregan. \n",
    "Usando inner join se hace interseccion de las dos tablas. \n",
    "\n",
    "2. ¿Por qué usamos LEFT JOIN en lugar de INNER JOIN para trips+zones? PISTA: ¿Qué pasaría si algunos viajes no tienen zona asignada?\n",
    "\n",
    "Porque al hacer left te aseguras que vas a mantener toda la informacion de los viajes agregando las zonas correspondientes a los mismos. Si hicieramos inner vamos a perder la informacion de los trips que no tienen zona asignada. \n",
    "\n",
    "3. ¿Qué problemas pueden surgir al hacer joins con datos de fechas? PISTA: Tipos de datos, formatos, zonas horarias\n",
    "\n",
    "• Diferencias en el tipo de dato, por ejemplo: string o datetime.\n",
    "• Formatos de fecha distintos, por ejemplo: YYYY-MM-DD o DD/MM/YYYY.\n",
    "• Valores nulos o fechas faltantes que pueden impedir el join.\n",
    "\n",
    "4. ¿Cuál es la ventaja de integrar múltiples fuentes de datos? PISTA: Análisis más rico, contexto completo, insights cruzados\n",
    "\n",
    "Nos permite realizar un análisis más completo y contextualizado. Además, se pueden cruzar variables de diferentes bases para descubrir patrones que no serían visibles en un solo dataset; esto enriquece la información y habilita conclusiones más profundos.\n",
    "\n",
    "5. ¿Qué insights de negocio obtuviste del análisis integrado? PISTA: Patrones por zona, impacto de eventos especiales, oportunidades\n",
    "\n",
    "Manhattan concentra la mayoría de los viajes, los viajes en Queens son más largos y costosos en promedio y el mejor revenue por km son de EWR. \n",
    "Hay diferencias claras en el revenue por kilómetro y en la tasa de propinas entre boroughs y los días especiales pueden tener impacto en la distancia y tarifa promedio.\n",
    "\n",
    "## BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddc262b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefect instalado y configurado\n",
      "   Versión: 3.4.14\n"
     ]
    }
   ],
   "source": [
    "import prefect\n",
    "from prefect import task, flow, get_run_logger\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"PREFECT_LOGGING_SERVER_ENABLED\"] = \"false\"\n",
    "\n",
    "print(\"Prefect instalado y configurado\")\n",
    "print(f\"   Versión: {prefect.__version__}\")\n",
    "\n",
    "# === TASKS SIMPLES PARA APRENDER PREFECT ===\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=10, name=\"Cargar Datos\")\n",
    "def cargar_datos(url: str, tipo: str) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Cargando {tipo} desde: {url}\")\n",
    "\n",
    "    tipo = tipo.lower().strip()\n",
    "    if tipo == \"parquet\":\n",
    "        # Requiere pyarrow instalado\n",
    "        return pd.read_parquet(url, engine=\"pyarrow\")\n",
    "    elif tipo == \"csv\":\n",
    "        # Maneja compresión (gzip) automáticamente\n",
    "        return pd.read_csv(url, encoding=\"utf-8\", low_memory=False, compression=\"infer\")\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo no soportado: {tipo} (usa 'csv' o 'parquet')\")\n",
    "\n",
    "\n",
    "@task(name=\"Hacer Join Simple\")\n",
    "def hacer_join_simple(trips: pd.DataFrame, zones: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Task para hacer join básico de trips + zones\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Haciendo join simple...\")\n",
    "\n",
    "    # Normalizar columnas\n",
    "    trips.columns = trips.columns.str.lower()  # convertir a minúsculas\n",
    "    zones.columns = zones.columns.str.lower()  # misma transformación\n",
    "\n",
    "    # Join básico\n",
    "    resultado = trips.merge(zones,   # método para unir DataFrames\n",
    "                             left_on='pickup_date',   # columna de pickup location en trips\n",
    "                             right_on='locationid',  # columna de location en zones\n",
    "                             how='left')       # tipo de join que mantiene todos los trips\n",
    "\n",
    "    logger.info(f\"Join completado: {len(resultado)} registros\")\n",
    "    return resultado\n",
    "\n",
    "@task(name=\"Análisis Rápido\")\n",
    "\n",
    "def analisis_rapido(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"Task para análisis básico\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Haciendo análisis básico...\")\n",
    "\n",
    "    # Stats simples\n",
    "    stats = {\n",
    "        'total_registros': len(data),\n",
    "        'boroughs': data['borough'].count().head(3).to_dict(),  # método para contar valores\n",
    "        'distancia_promedio': round(data['trip_distance'].mean(), 2),  # método para promedio\n",
    "        'tarifa_promedio': round(data['total_amount'].mean(), 2)  # método para promedio\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Análisis completado: {stats['total_registros']} registros\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c58f71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FLOW PRINCIPAL (EL PIPELINE COMPLETO) ===\n",
    "\n",
    "@flow(name=\"Pipeline Simple NYC Taxi\")\n",
    "def pipeline_taxi_simple():\n",
    "    \"\"\"\n",
    "    Flow simple que conecta todos los tasks\n",
    "    \"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Iniciando pipeline simple...\")\n",
    "\n",
    "    # URLs de datos\n",
    "    trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "    zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "\n",
    "    # PASO 1: Cargar datos (con retry automático si falla)\n",
    "    logger.info(\"Paso 1: Cargando datos...\")\n",
    "    trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n",
    "    zones = cargar_datos(zones_url, \"csv\")  # tipo de datos zones\n",
    "\n",
    "    # PASO 2: Hacer join\n",
    "    logger.info(\"Paso 2: Haciendo join...\")\n",
    "    data_unida = hacer_join_simple(trips, zones)\n",
    "\n",
    "    # PASO 3: Análisis básico\n",
    "    logger.info(\"Paso 3: Analizando...\")\n",
    "    resultados = analisis_rapido(data_unida)\n",
    "\n",
    "    # PASO 4: Mostrar resultados\n",
    "    logger.info(\"Pipeline completado!\")\n",
    "    logger.info(f\"Resultados: {resultados}\")\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d72e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando pipeline simple...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:37.568 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'honored-wolverine'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'honored-wolverine'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'Pipeline Simple NYC Taxi'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:37.568 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'honored-wolverine'\u001b[0m - Beginning flow run\u001b[35m 'honored-wolverine'\u001b[0m for flow\u001b[1;35m 'Pipeline Simple NYC Taxi'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:37.572 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'honored-wolverine'</span> - Iniciando pipeline simple...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:37.572 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'honored-wolverine'\u001b[0m - Iniciando pipeline simple...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:37.579 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'honored-wolverine'</span> - Paso 1: Cargando datos...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:37.579 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'honored-wolverine'\u001b[0m - Paso 1: Cargando datos...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:38.316 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:38.316 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: \u001b[94mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:39.693 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('No type extension with name arrow.py_extension_type found') - Retry 1/3 will start 10 second(s) from now\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:39.693 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('No type extension with name arrow.py_extension_type found') - Retry 1/3 will start 10 second(s) from now\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:49.703 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:49.703 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: \u001b[94mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:49.711 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retry 2/3 will start 10 second(s) from now\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:49.711 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retry 2/3 will start 10 second(s) from now\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:59.726 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:59.726 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: \u001b[94mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:45:59.735 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retry 3/3 will start 10 second(s) from now\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:45:59.735 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retry 3/3 will start 10 second(s) from now\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:46:09.744 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:46:09.744 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-ab5' - Cargando parquet desde: \u001b[94mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:46:09.751 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retries are exhausted\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Estudiante UCU\\AppData\\Local\\Temp\\ipykernel_33372\\1823903112.py\", line 21, in cargar_datos\n",
       "    return pd.read_parquet(url, engine=\"pyarrow\")\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 651, in read_parquet\n",
       "    impl = get_engine(engine)\n",
       "           ^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 78, in get_engine\n",
       "    return PyArrowImpl()\n",
       "           ^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 169, in __init__\n",
       "    import pandas.core.arrays.arrow.extension_types  # pyright: ignore[reportUnusedImport] # noqa: F401\n",
       "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py\", line 59, in &lt;module&gt;\n",
       "    pyarrow.register_extension_type(_period_type)\n",
       "  File \"pyarrow\\\\types.pxi\", line 2226, in pyarrow.lib.register_extension_type\n",
       "  File \"pyarrow\\\\error.pxi\", line 92, in pyarrow.lib.check_status\n",
       "pyarrow.lib.ArrowKeyError: A type extension with name pandas.period already defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:46:09.751 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Cargar Datos-ab5' - Task run failed with exception: ArrowKeyError('A type extension with name pandas.period already defined') - Retries are exhausted\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Estudiante UCU\\AppData\\Local\\Temp\\ipykernel_33372\\1823903112.py\", line 21, in cargar_datos\n",
       "    return pd.read_parquet(url, engine=\"pyarrow\")\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 651, in read_parquet\n",
       "    impl = get_engine(engine)\n",
       "           ^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 78, in get_engine\n",
       "    return PyArrowImpl()\n",
       "           ^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 169, in __init__\n",
       "    import pandas.core.arrays.arrow.extension_types  # pyright: ignore[reportUnusedImport] # noqa: F401\n",
       "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py\", line 59, in <module>\n",
       "    pyarrow.register_extension_type(_period_type)\n",
       "  File \"pyarrow\\\\types.pxi\", line 2226, in pyarrow.lib.register_extension_type\n",
       "  File \"pyarrow\\\\error.pxi\", line 92, in pyarrow.lib.check_status\n",
       "pyarrow.lib.ArrowKeyError: A type extension with name pandas.period already defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:46:09.788 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Cargar Datos-ab5' - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>('Task run encountered an exception ArrowKeyError: A type extension with name pandas.period already defined')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:46:09.788 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Cargar Datos-ab5' - Finished in state \u001b[38;5;160mFailed\u001b[0m('Task run encountered an exception ArrowKeyError: A type extension with name pandas.period already defined')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:46:09.794 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'honored-wolverine'</span> - Encountered exception during execution: ArrowKeyError('A type extension with name pandas.period already defined')\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py\", line 782, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py\", line 1397, in run_flow_sync\n",
       "    engine.call_flow_fn()\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py\", line 802, in call_flow_fn\n",
       "    result = call_with_parameters(self.flow.fn, self.parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Estudiante UCU\\AppData\\Local\\Temp\\ipykernel_33372\\3170529249.py\", line 17, in pipeline_taxi_simple\n",
       "    trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n",
       "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\tasks.py\", line 1139, in __call__\n",
       "    return run_task(\n",
       "           ^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1732, in run_task\n",
       "    return run_task_sync(**kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1507, in run_task_sync\n",
       "    return engine.state if return_type == \"state\" else engine.result()\n",
       "                                                       ^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 494, in result\n",
       "    raise self._raised\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Estudiante UCU\\AppData\\Local\\Temp\\ipykernel_33372\\1823903112.py\", line 21, in cargar_datos\n",
       "    return pd.read_parquet(url, engine=\"pyarrow\")\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 651, in read_parquet\n",
       "    impl = get_engine(engine)\n",
       "           ^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 78, in get_engine\n",
       "    return PyArrowImpl()\n",
       "           ^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 169, in __init__\n",
       "    import pandas.core.arrays.arrow.extension_types  # pyright: ignore[reportUnusedImport] # noqa: F401\n",
       "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py\", line 59, in &lt;module&gt;\n",
       "    pyarrow.register_extension_type(_period_type)\n",
       "  File \"pyarrow\\\\types.pxi\", line 2226, in pyarrow.lib.register_extension_type\n",
       "  File \"pyarrow\\\\error.pxi\", line 92, in pyarrow.lib.check_status\n",
       "pyarrow.lib.ArrowKeyError: A type extension with name pandas.period already defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:46:09.794 | \u001b[38;5;160mERROR\u001b[0m   | Flow run\u001b[35m 'honored-wolverine'\u001b[0m - Encountered exception during execution: ArrowKeyError('A type extension with name pandas.period already defined')\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py\", line 782, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py\", line 1397, in run_flow_sync\n",
       "    engine.call_flow_fn()\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py\", line 802, in call_flow_fn\n",
       "    result = call_with_parameters(self.flow.fn, self.parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Estudiante UCU\\AppData\\Local\\Temp\\ipykernel_33372\\3170529249.py\", line 17, in pipeline_taxi_simple\n",
       "    trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n",
       "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\tasks.py\", line 1139, in __call__\n",
       "    return run_task(\n",
       "           ^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1732, in run_task\n",
       "    return run_task_sync(**kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1507, in run_task_sync\n",
       "    return engine.state if return_type == \"state\" else engine.result()\n",
       "                                                       ^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 494, in result\n",
       "    raise self._raised\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "           ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\Estudiante UCU\\AppData\\Local\\Temp\\ipykernel_33372\\1823903112.py\", line 21, in cargar_datos\n",
       "    return pd.read_parquet(url, engine=\"pyarrow\")\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 651, in read_parquet\n",
       "    impl = get_engine(engine)\n",
       "           ^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 78, in get_engine\n",
       "    return PyArrowImpl()\n",
       "           ^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py\", line 169, in __init__\n",
       "    import pandas.core.arrays.arrow.extension_types  # pyright: ignore[reportUnusedImport] # noqa: F401\n",
       "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"c:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py\", line 59, in <module>\n",
       "    pyarrow.register_extension_type(_period_type)\n",
       "  File \"pyarrow\\\\types.pxi\", line 2226, in pyarrow.lib.register_extension_type\n",
       "  File \"pyarrow\\\\error.pxi\", line 92, in pyarrow.lib.check_status\n",
       "pyarrow.lib.ArrowKeyError: A type extension with name pandas.period already defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">11:46:09.846 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'honored-wolverine'</span> - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>('Flow run encountered an exception: ArrowKeyError: A type extension with name pandas.period already defined')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "11:46:09.846 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'honored-wolverine'\u001b[0m - Finished in state \u001b[38;5;160mFailed\u001b[0m('Flow run encountered an exception: ArrowKeyError: A type extension with name pandas.period already defined')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowKeyError",
     "evalue": "A type extension with name pandas.period already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEjecutando pipeline simple...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Ejecutar el flow\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m resultado = \u001b[43mpipeline_taxi_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nombre de la función del flow\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRESULTADOS FINALES:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Total registros: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresultado[\u001b[33m'\u001b[39m\u001b[33mtotal_registros\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flows.py:1702\u001b[39m, in \u001b[36mFlow.__call__\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m track_viz_task(\u001b[38;5;28mself\u001b[39m.isasync, \u001b[38;5;28mself\u001b[39m.name, parameters)\n\u001b[32m   1700\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprefect\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflow_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_flow\n\u001b[32m-> \u001b[39m\u001b[32m1702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_flow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py:1554\u001b[39m, in \u001b[36mrun_flow\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, error_logger, context)\u001b[39m\n\u001b[32m   1552\u001b[39m         ret_val = run_flow_async(**kwargs)\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m         ret_val = \u001b[43mrun_flow_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (Abort, Pause):\n\u001b[32m   1556\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py:1399\u001b[39m, in \u001b[36mrun_flow_sync\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1396\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m   1397\u001b[39m             engine.call_flow_fn()\n\u001b[32m-> \u001b[39m\u001b[32m1399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py:361\u001b[39m, in \u001b[36mFlowRunEngine.result\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# This is a fall through case which leans on the existing state result mechanics to get the\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# return value. This is necessary because we currently will return a State object if the\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# the State was Prefect-created.\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# TODO: Remove the need to get the result from a State except in cases where the return value\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# is a State object.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py:782\u001b[39m, in \u001b[36mFlowRunEngine.run_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m timeout_context(\n\u001b[32m    776\u001b[39m         seconds=\u001b[38;5;28mself\u001b[39m.flow.timeout_seconds,\n\u001b[32m    777\u001b[39m         timeout_exc_type=FlowRunTimeoutError,\n\u001b[32m    778\u001b[39m     ):\n\u001b[32m    779\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.debug(\n\u001b[32m    780\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuting flow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for flow run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.flow_run.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    781\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_timeout(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py:1397\u001b[39m, in \u001b[36mrun_flow_sync\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m engine.is_running():\n\u001b[32m   1396\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m             \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_flow_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\flow_engine.py:802\u001b[39m, in \u001b[36mFlowRunEngine.call_flow_fn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_flow_fn()\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     result = \u001b[43mcall_with_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    803\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_success(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py:210\u001b[39m, in \u001b[36mcall_with_parameters\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03mCall a function with parameters extracted with `get_call_parameters`\u001b[39;00m\n\u001b[32m    204\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \u001b[33;03mthe args/kwargs using `parameters_to_positional_and_keyword` directly\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mpipeline_taxi_simple\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# PASO 1: Cargar datos (con retry automático si falla)\u001b[39;00m\n\u001b[32m     16\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mPaso 1: Cargando datos...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m trips = \u001b[43mcargar_datos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrips_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# tipo de datos trips\u001b[39;00m\n\u001b[32m     18\u001b[39m zones = cargar_datos(zones_url, \u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# tipo de datos zones\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# PASO 2: Hacer join\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\tasks.py:1139\u001b[39m, in \u001b[36mTask.__call__\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m track_viz_task(\n\u001b[32m   1134\u001b[39m         \u001b[38;5;28mself\u001b[39m.isasync, \u001b[38;5;28mself\u001b[39m.name, parameters, \u001b[38;5;28mself\u001b[39m.viz_return_value\n\u001b[32m   1135\u001b[39m     )\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprefect\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtask_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_task\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py:1732\u001b[39m, in \u001b[36mrun_task\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_task_async(**kwargs)\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_task_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py:1507\u001b[39m, in \u001b[36mrun_task_sync\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1500\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1501\u001b[39m             engine.asset_context(),\n\u001b[32m   1502\u001b[39m             engine.run_context(),\n\u001b[32m   1503\u001b[39m             engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1504\u001b[39m         ):\n\u001b[32m   1505\u001b[39m             engine.call_task_fn(txn)\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py:494\u001b[39m, in \u001b[36mSyncTaskRunEngine.result\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# if the task raised an exception, raise it\u001b[39;00m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    496\u001b[39m     \u001b[38;5;66;03m# otherwise, return the exception\u001b[39;00m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py:869\u001b[39m, in \u001b[36mSyncTaskRunEngine.run_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    866\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_cancelled():\n\u001b[32m    867\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m CancelledError(\u001b[33m\"\u001b[39m\u001b[33mTask run cancelled by the task runner\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    871\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_timeout(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py:1505\u001b[39m, in \u001b[36mrun_task_sync\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1499\u001b[39m         run_coro_as_sync(engine.wait_until_ready())\n\u001b[32m   1500\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1501\u001b[39m             engine.asset_context(),\n\u001b[32m   1502\u001b[39m             engine.run_context(),\n\u001b[32m   1503\u001b[39m             engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1504\u001b[39m         ):\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m             \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_task_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\task_engine.py:886\u001b[39m, in \u001b[36mSyncTaskRunEngine.call_task_fn\u001b[39m\u001b[34m(self, transaction)\u001b[39m\n\u001b[32m    884\u001b[39m     result = transaction.read()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m     result = \u001b[43mcall_with_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[38;5;28mself\u001b[39m.handle_success(result, transaction=transaction)\n\u001b[32m    888\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prefect\\utilities\\callables.py:210\u001b[39m, in \u001b[36mcall_with_parameters\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03mCall a function with parameters extracted with `get_call_parameters`\u001b[39;00m\n\u001b[32m    204\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \u001b[33;03mthe args/kwargs using `parameters_to_positional_and_keyword` directly\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mcargar_datos\u001b[39m\u001b[34m(url, tipo)\u001b[39m\n\u001b[32m     18\u001b[39m tipo = tipo.lower().strip()\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tipo == \u001b[33m\"\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Requiere pyarrow instalado\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tipo == \u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Maneja compresión (gzip) automáticamente\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_csv(url, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m, low_memory=\u001b[38;5;28;01mFalse\u001b[39;00m, compression=\u001b[33m\"\u001b[39m\u001b[33minfer\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:651\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    500\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    508\u001b[39m     **kwargs,\n\u001b[32m    509\u001b[39m ) -> DataFrame:\n\u001b[32m    510\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    512\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    654\u001b[39m         msg = (\n\u001b[32m    655\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:78\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyArrowImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:169\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# register the type with a dummy instance\u001b[39;00m\n\u001b[32m     58\u001b[39m _period_type = ArrowPeriodType(\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_period_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mArrowIntervalType\u001b[39;00m(pyarrow.ExtensionType):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subtype, closed: IntervalClosedType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# attributes need to be set first before calling\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# super init (as that calls serialize)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\types.pxi:2226\u001b[39m, in \u001b[36mpyarrow.lib.register_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Estudiante UCU\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: A type extension with name pandas.period already defined"
     ]
    }
   ],
   "source": [
    "# === EJECUTAR EL PIPELINE ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Ejecutando pipeline simple...\")\n",
    "\n",
    "    # Ejecutar el flow\n",
    "    resultado = pipeline_taxi_simple()  # nombre de la función del flow\n",
    "\n",
    "    print(\"\\nRESULTADOS FINALES:\")\n",
    "    print(f\"   Total registros: {resultado['total_registros']:,}\")\n",
    "    print(f\"   Distancia promedio: {resultado['distancia_promedio']} millas\")\n",
    "    print(f\"   Tarifa promedio: ${resultado['tarifa_promedio']}\")\n",
    "    print(\"\\nTop 3 Boroughs:\")\n",
    "    for borough, count in resultado['top_boroughs'].items():  # clave del diccionario que contiene boroughs\n",
    "        print(f\"   {borough}: {count:,} viajes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31bdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTES - Código normal:\n",
    "def cargar_datos(url):\n",
    "    return pd.read_csv(url)  # Si falla, todo se rompe\n",
    "\n",
    "# DESPUÉS - Con Prefect:\n",
    "@task(retries=2)\n",
    "def cargar_datos(url):\n",
    "    return pd.read_csv(url)  # Si falla, lo intenta 2 veces más"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e2306",
   "metadata": {},
   "source": [
    "# PREGUNTAS BONUS\n",
    "\n",
    "1. ¿Qué ventaja tiene usar @task en lugar de una función normal? PISTA: ¿Qué pasa si la carga de datos falla temporalmente?\n",
    "\n",
    "    Una función normal en Python se ejecuta sin ningún control extra.\n",
    "\n",
    "    Cuando usas @task:\n",
    "\n",
    "    - Podés definir reintentos automáticos si falla por algo temporal (ej: red caída al bajar datos).\n",
    "\n",
    "    - Podés definir cortes por tiempo (si un paso tarda demasiado, Prefect lo corta).\n",
    "\n",
    "    - Tenés logs integrados en la interfaz de Prefect.\n",
    "\n",
    "    - Cada task queda orquestado y monitoreado: podés ver qué falló y reintentar sólo ese paso.\n",
    "\n",
    "2. ¿Para qué sirve el @flow decorator? PISTA: ¿Cómo conecta y organiza los tasks?\n",
    "    El @flow define un pipeline completo, que organiza y conecta varios @task.\n",
    "\n",
    "    Le dice a Prefect: “Esto no es sólo un script de Python, es un flujo de trabajo con dependencias y monitoreo”.\n",
    "\n",
    "    Permite:\n",
    "\n",
    "    - Ejecutar tasks en orden y pasar resultados de uno a otro.\n",
    "\n",
    "    - Monitorear el flow run entero.\n",
    "\n",
    "3. ¿En qué casos reales usarías esto? PISTA: Reportes diarios, análisis automáticos, pipelines de ML\n",
    "\n",
    "    Reportes diarios: Automatizar que cada mañana se bajen datos de ventas, se limpien y se envíen reportes a un dashboard.\n",
    "\n",
    "    Análisis automáticos: Procesar logs de usuarios, detectar anomalías o generar alertas de fraude sin intervención manual.\n",
    "\n",
    "    Pipelines de ML: cargar dataset - limpiar/preprocesar - entrenar modelo - guardar métricas "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
